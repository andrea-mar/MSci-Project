{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP for Text Classfication to select only the ASD relevant subreddits - see discussion in *reddit_data.ipynb* : \n",
    "\n",
    "'From the above analysis we can see that the first 10 subreddits , by number of subscribers, do not seem to be related to ASD. \n",
    "\n",
    "=> Use NLP to filter out the ASD relavant subreddits : display_name + title + public_description + header_title + description (Text classification task - target categories: ASD vs Other. ) - Manually annotate 40% out of 534 subreddits -> train different models -> choose the best one to classify the rest 60% and get the final subreddits list.\n",
    "\n",
    "As the private subreddits' data can't be accessed, these will be excluded from the analisys. The subreddits dataset will only inlcude those that are listed as 'public' or 'restricted' (= everyone can read the data but only certain people can post).\n",
    "\n",
    "Features used: display_name + title + public_description + header_title + description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline for NLP:\n",
    "\n",
    "1. get data (40% asd_final_rows_list.csv) and annotate it \n",
    "2. text extraction and cleanup - merge relevant information into one text column, remove punctuation/spelling mistakes etc. \n",
    "3. pre-processing:\n",
    "    3.1 split text into tokens/words (=sentence segmentation/ tokanisation NLTK or Spcy or AutoTokenizer if using transformers)\n",
    "    3.3 stemming (=remove prefixes and sufixes) and lemmatization (=get the base word - ext ate becomes (to)eat)\n",
    "4. feature engineering (= convert text / document into vector)\n",
    "    count-vector\n",
    "    tf-idf\n",
    "    one-hot encode - not really used due to size and sparcity problems\n",
    "    word/token-emebeding\n",
    "5. apply classifier ( any machine learning model classifier ) - use gridSearchCV to see which is the best performing classifier ?\n",
    "6. evaluate  model: Accuracy, Precision, Recall, F1 score ( if not good , go back to preprocessing step )\n",
    "7. Deploy\n",
    "8. Monitor and update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get data  - whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 534 entries, 0 to 533\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0             534 non-null    int64  \n",
      " 1   restrict_posting       534 non-null    bool   \n",
      " 2   display_name           534 non-null    object \n",
      " 3   title                  534 non-null    object \n",
      " 4   display_name_prefixed  534 non-null    object \n",
      " 5   subscribers            534 non-null    int64  \n",
      " 6   name                   534 non-null    object \n",
      " 7   public_description     506 non-null    object \n",
      " 8   community_reviewed     534 non-null    bool   \n",
      " 9   created                534 non-null    float64\n",
      " 10  subreddit_type         534 non-null    object \n",
      " 11  id                     534 non-null    object \n",
      " 12  over18                 534 non-null    bool   \n",
      " 13  header_title           81 non-null     object \n",
      " 14  description            393 non-null    object \n",
      " 15  url                    534 non-null    object \n",
      " 16  created_utc            534 non-null    float64\n",
      " 17  _path                  534 non-null    object \n",
      " 18  lang                   534 non-null    object \n",
      "dtypes: bool(3), float64(2), int64(2), object(12)\n",
      "memory usage: 68.4+ KB\n"
     ]
    }
   ],
   "source": [
    "asd_subs_df = pd.read_csv('asd_final_rows_list.csv')\n",
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 534 entries, 0 to 533\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  534 non-null    object\n",
      " 1   display_name        534 non-null    object\n",
      " 2   title               534 non-null    object\n",
      " 3   public_description  506 non-null    object\n",
      " 4   header_title        81 non-null     object\n",
      " 5   description         393 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 25.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# select from the dataset only the text features neccesary for topic modeling + id \n",
    "# Features: display_name + title + public_description + header_title + description \n",
    "asd_subs_df = asd_subs_df[['id','display_name', 'title', 'public_description', 'header_title', 'description']]\n",
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>title</th>\n",
       "      <th>public_description</th>\n",
       "      <th>header_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Awareness</td>\n",
       "      <td>A place for kind people who want to spread awa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>AspiePositive: by aspies, for aspies | no hate...</td>\n",
       "      <td>If you have Asperger's, you are welcome. If yo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>**The Golden Rule: Be excellent to each other....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Trump Autism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "\n",
       "                                               title  \\\n",
       "0                                   Autism Awareness   \n",
       "1  AspiePositive: by aspies, for aspies | no hate...   \n",
       "2                                       Trump Autism   \n",
       "\n",
       "                                  public_description header_title  \\\n",
       "0  A place for kind people who want to spread awa...          NaN   \n",
       "1  If you have Asperger's, you are welcome. If yo...          NaN   \n",
       "2                                       Trump Autism          NaN   \n",
       "\n",
       "                                         description  \n",
       "0                                                NaN  \n",
       "1  **The Golden Rule: Be excellent to each other....  \n",
       "2                                       Trump Autism  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# public_description, header_title, description have missing data\n",
    "# in the reddit_data.ipynb the missing data appears as an empty string, meaning the subreddit does not have any text added to these sections\n",
    "# all NaN values will be replaced with an empty string \n",
    "# the text in each column will be agregated into one single text document as to include as much relevant words as possible for each subreddit\n",
    "asd_subs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                      0\n",
       "display_name            0\n",
       "title                   0\n",
       "public_description     28\n",
       "header_title          453\n",
       "description           141\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check missing data\n",
    "asd_subs_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 534 entries, of which 28 are missing 'public_description' and 141 are missing 'description' and 453 are missing 'header_title'.\n",
    "\n",
    "display_name and title have no missing data.\n",
    "\n",
    "All text columns will be aggregated into one large text document ( to include all relevant words available for each subreddit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Clean up and text extraction - whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>title</th>\n",
       "      <th>public_description</th>\n",
       "      <th>header_title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Awareness</td>\n",
       "      <td>A place for kind people who want to spread awa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>AspiePositive: by aspies, for aspies | no hate...</td>\n",
       "      <td>If you have Asperger's, you are welcome. If yo...</td>\n",
       "      <td></td>\n",
       "      <td>**The Golden Rule: Be excellent to each other....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td></td>\n",
       "      <td>Trump Autism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "\n",
       "                                               title  \\\n",
       "0                                   Autism Awareness   \n",
       "1  AspiePositive: by aspies, for aspies | no hate...   \n",
       "2                                       Trump Autism   \n",
       "\n",
       "                                  public_description header_title  \\\n",
       "0  A place for kind people who want to spread awa...                \n",
       "1  If you have Asperger's, you are welcome. If yo...                \n",
       "2                                       Trump Autism                \n",
       "\n",
       "                                         description  \n",
       "0                                                     \n",
       "1  **The Golden Rule: Be excellent to each other....  \n",
       "2                                       Trump Autism  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NaN values with empty string ''\n",
    "# NaN valuse are only observed for text columns: public_description, header_title, description\n",
    "asd_subs_df = asd_subs_df.fillna('')\n",
    "asd_subs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 534 entries, 0 to 533\n",
      "Data columns (total 6 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  534 non-null    object\n",
      " 1   display_name        534 non-null    object\n",
      " 2   title               534 non-null    object\n",
      " 3   public_description  534 non-null    object\n",
      " 4   header_title        534 non-null    object\n",
      " 5   description         534 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 25.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# no more null values \n",
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>title</th>\n",
       "      <th>public_description</th>\n",
       "      <th>header_title</th>\n",
       "      <th>description</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Awareness</td>\n",
       "      <td>A place for kind people who want to spread awa...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>AutismRepresentation Autism Awareness A place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>AspiePositive: by aspies, for aspies | no hate...</td>\n",
       "      <td>If you have Asperger's, you are welcome. If yo...</td>\n",
       "      <td></td>\n",
       "      <td>**The Golden Rule: Be excellent to each other....</td>\n",
       "      <td>AspiePositive AspiePositive: by aspies, for as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td></td>\n",
       "      <td>Trump Autism</td>\n",
       "      <td>TrumpAutism Trump Autism Trump Autism  Trump A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "\n",
       "                                               title  \\\n",
       "0                                   Autism Awareness   \n",
       "1  AspiePositive: by aspies, for aspies | no hate...   \n",
       "2                                       Trump Autism   \n",
       "\n",
       "                                  public_description header_title  \\\n",
       "0  A place for kind people who want to spread awa...                \n",
       "1  If you have Asperger's, you are welcome. If yo...                \n",
       "2                                       Trump Autism                \n",
       "\n",
       "                                         description  \\\n",
       "0                                                      \n",
       "1  **The Golden Rule: Be excellent to each other....   \n",
       "2                                       Trump Autism   \n",
       "\n",
       "                                                text  \n",
       "0  AutismRepresentation Autism Awareness A place ...  \n",
       "1  AspiePositive AspiePositive: by aspies, for as...  \n",
       "2  TrumpAutism Trump Autism Trump Autism  Trump A...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join text columns\n",
    "asd_subs_df[\"text\"] = asd_subs_df[[\"display_name\", \"title\", 'public_description', 'header_title', 'description']].apply(\" \".join, axis='columns')\n",
    "asd_subs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 534 entries, 0 to 533\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   id                  534 non-null    object\n",
      " 1   display_name        534 non-null    object\n",
      " 2   title               534 non-null    object\n",
      " 3   public_description  534 non-null    object\n",
      " 4   header_title        534 non-null    object\n",
      " 5   description         534 non-null    object\n",
      " 6   text                534 non-null    object\n",
      "dtypes: object(7)\n",
      "memory usage: 29.3+ KB\n"
     ]
    }
   ],
   "source": [
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>AutismRepresentation Autism Awareness A place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>AspiePositive AspiePositive: by aspies, for as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>TrumpAutism Trump Autism Trump Autism  Trump A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "\n",
       "                                                text  \n",
       "0  AutismRepresentation Autism Awareness A place ...  \n",
       "1  AspiePositive AspiePositive: by aspies, for as...  \n",
       "2  TrumpAutism Trump Autism Trump Autism  Trump A...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the individual text columns for easier visualisation\n",
    "# only the text column will be used for the analysis\n",
    "# keep id and display_name - to identify subreddits\n",
    "asd_subs_df.drop(['title', 'public_description', 'header_title', 'description'], axis=1, inplace=True)\n",
    "asd_subs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327    autism__friends autism__friends a safe place f...\n",
       "381    videos /r/videos Reddit's main subreddit for v...\n",
       "482    mentalhealth Mental Health The Mental Health s...\n",
       "55     offmychest Off My Chest | A Safe Community for...\n",
       "273    AutismAndAddiction AutismAndAddiction A place ...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "97     IAmA IAmA I Am A, where the mundane becomes fa...\n",
       "90     WritteND WritteND A space for Neurodivergent w...\n",
       "204    asptrees asptrees the community for discussing...\n",
       "70     Autisme_France Autisme_France Groupe qui conce...\n",
       "478    Parenting Reddit Parenting - For those with ki...\n",
       "395    genetics Genetics, genes, and genomes For disc...\n",
       "472    Aspergians Aspergians A subreddit by and for s...\n",
       "50     aspieselfies aspieselfies a place for people w...\n",
       "306    AutisticPilking AutisticPilking Neco Arc is yo...\n",
       "115    TheAspieWorld The Aspie World - Aspergers & Au...\n",
       "109    CoronavirusUK CoronavirusUK Spreading news, ad...\n",
       "132    AskMeAnythingIAnswer We will try to answer any...\n",
       "351    AskTeenGirls r/AskTeenGirls: Ask girls questio...\n",
       "227    CasaAutistic CasaAutistic la casa autistic es ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invesitgate data further ( check text patterns that could be cleaned further )\n",
    "asd_subs_df['text'].sample(n=20, random_state=7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms appear in cammelCase - words capitalized and joined together - ex. *AutismAndAddiction*; Some terms are separated by _ : ex. *autism__friends*. We can separate these words by white spaces using regular expressions. However, some words are joined together with no clear delimitation (ex. *nevergrewup* ). In this situation we can't separate the words, but the deep leaning approach should be able to tokenize this text in a relevant way as these models split the text into single characters or groups of characters, or use bytes instead of characters/words ( see : Unigrams (Gasparetto et al., 2022)). To clean the data in this section cammelCase, dashes and underscores will be replaced by white spaces in the dataset. Concatenated strings of words with no clear delimitation will be left unchanged in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find and replace all cammelCase, dash and undreline joined words\n",
    "import re\n",
    "def split_words_by_space(text):\n",
    "    # split camelCase\n",
    "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text) \n",
    "    # replace underscores and dashes with spaces\n",
    "    text = re.sub(r'[_-]+', ' ', text)  \n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text words by spaces\n",
    "asd_subs_df['text'] = asd_subs_df['text'].apply(split_words_by_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327    autism friends autism friends a safe place for...\n",
       "381    videos /r/videos Reddit's main subreddit for v...\n",
       "482    mentalhealth Mental Health The Mental Health s...\n",
       "55     offmychest Off My Chest | A Safe Community for...\n",
       "273    Autism And Addiction Autism And Addiction A pl...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "97     IAm A IAm A I Am A, where the mundane becomes ...\n",
       "90     Writte ND Writte ND A space for Neurodivergent...\n",
       "204    asptrees asptrees the community for discussing...\n",
       "70     Autisme France Autisme France Groupe qui conce...\n",
       "478    Parenting Reddit Parenting   For those with ki...\n",
       "395    genetics Genetics, genes, and genomes For disc...\n",
       "472    Aspergians Aspergians A subreddit by and for s...\n",
       "50     aspieselfies aspieselfies a place for people w...\n",
       "306    Autistic Pilking Autistic Pilking Neco Arc is ...\n",
       "115    The Aspie World The Aspie World   Aspergers & ...\n",
       "109    Coronavirus UK Coronavirus UK Spreading news, ...\n",
       "132    Ask Me Anything IAnswer We will try to answer ...\n",
       "351    Ask Teen Girls r/Ask Teen Girls: Ask girls que...\n",
       "227    Casa Autistic Casa Autistic la casa autistic e...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_df['text'].sample(n=20, random_state=7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that some of the subreddis are not in english (227-CasaAutistic, 70-Autisme_France)\n",
    "\n",
    "Use langdetect to identify the language of the subreddits\n",
    "(https://pypi.org/project/langdetect/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect langage of subreddits text and save it in a 'lang' column\n",
    "from langdetect import detect\n",
    "asd_subs_df['lang'] = asd_subs_df['text'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 18 rows are identified as not being english\n",
    "len(asd_subs_df[asd_subs_df['lang'] != 'en'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Autism Trump Autism Trump Autism  Trump ...</td>\n",
       "      <td>lv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Aspergers After Dark Aspergers After Dark Migr...</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>autismogang  autismogang</td>\n",
       "      <td>tl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Autisme France Autisme France Groupe qui conce...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Asperger En Espanol Asperger En Espanol</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Vidar Autism Vidar Autism Autistic Vidar minec...</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>aspergerinterests Asperger Interests</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Casa Autistic Casa Autistic la casa autistic e...</td>\n",
       "      <td>ca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Autistic Pride2 Autistic Pride2</td>\n",
       "      <td>ro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>girl parents autism girl parents autism</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Autism Art Autism Art welcome!</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Autism Gamers Autism Gamers</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>Masked Autism Masked Autism</td>\n",
       "      <td>et</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Real Autists Romania Real Autists Romania Salu...</td>\n",
       "      <td>ro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Asperger Latam Asperger Latam Una comunidad en...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Asper Minismo Asper Minismo El Asperminismo es...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Aspergers BR Aspergers BR Uma comunidade volta...</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text lang\n",
       "2    Trump Autism Trump Autism Trump Autism  Trump ...   lv\n",
       "16   Aspergers After Dark Aspergers After Dark Migr...   da\n",
       "56                        autismogang  autismogang       tl\n",
       "70   Autisme France Autisme France Groupe qui conce...   fr\n",
       "91          Asperger En Espanol Asperger En Espanol      es\n",
       "93   Vidar Autism Vidar Autism Autistic Vidar minec...   de\n",
       "214           aspergerinterests Asperger Interests       no\n",
       "227  Casa Autistic Casa Autistic la casa autistic e...   ca\n",
       "263                 Autistic Pride2 Autistic Pride2      ro\n",
       "283         girl parents autism girl parents autism      fr\n",
       "291                  Autism Art Autism Art welcome!      de\n",
       "300                     Autism Gamers Autism Gamers      de\n",
       "345                     Masked Autism Masked Autism      et\n",
       "388  Real Autists Romania Real Autists Romania Salu...   ro\n",
       "430  Asperger Latam Asperger Latam Una comunidad en...   es\n",
       "487  Asper Minismo Asper Minismo El Asperminismo es...   es\n",
       "512  Aspergers BR Aspergers BR Uma comunidade volta...   pt"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect rows manually to confirm language\n",
    "# set the maximum column width to display all content\n",
    "# pd.set_option('display.max_colwidth', 1)\n",
    "asd_subs_df[asd_subs_df['lang'] != 'en'][['text', 'lang']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the rows above, the following row are confirmed as not being in English: \n",
    "56, 70, 91, 227, 388, 430, 487, 512\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 526 entries, 0 to 533\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            526 non-null    object\n",
      " 1   display_name  526 non-null    object\n",
      " 2   text          526 non-null    object\n",
      " 3   lang          526 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 20.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# drop the non-english rows\n",
    "# 526 subreddits left in the dataset\n",
    "non_en_rows_indices = [56, 70, 91, 227, 388, 430, 487, 512]\n",
    "asd_subs_df = asd_subs_df.drop(non_en_rows_indices)\n",
    "asd_subs_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205    autismmemes autism memes Meme for people with\\...\n",
       "488    ASDcareers The Careers of People with ASD Livi...\n",
       "231    Gamingcirclejerk Gaming Circlejerk   Don Chead...\n",
       "104    autisticpeople Autism Spectrum Disorder News, ...\n",
       "263                   Autistic Pride2 Autistic Pride2   \n",
       "492    Baby Bumps Baby Bumps A place for pregnant red...\n",
       "236    hypersensitivity Hypersensitivity A place for ...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "323    Everything Science Everything Science /r/Every...\n",
       "506    aplusguide A+ Guide A+ Guide is a place for po...\n",
       "66     Neurodivergent LGBTQ Neurodivergent LGBTQ Neur...\n",
       "293    The Owl House The Owl House A subreddit for th...\n",
       "317    traaaaaaannnnnnnnnns If youre memes and im mem...\n",
       "529    Savant Savant This subreddit is dedicated to t...\n",
       "278    Debate Vaccines Debate Vaccines Debate and dis...\n",
       "51     intj analytical, conceptual and objective For ...\n",
       "216    gaspies A place for gay aspies For the aspies ...\n",
       "219    Council Of Autism Council Of Autism This is no...\n",
       "190    BPD Borderline Personality Disorder r/BPD is a...\n",
       "207    wallstreetbets wallstreetbets Like 4chan found...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_df['text'].sample(n=20, random_state=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205    autismmemes autism memes Meme for people with ...\n",
       "488    ASDcareers The Careers of People with ASD Livi...\n",
       "231    Gamingcirclejerk Gaming Circlejerk   Don Chead...\n",
       "104    autisticpeople Autism Spectrum Disorder News, ...\n",
       "263                   Autistic Pride2 Autistic Pride2   \n",
       "492    Baby Bumps Baby Bumps A place for pregnant red...\n",
       "236    hypersensitivity Hypersensitivity A place for ...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "323    Everything Science Everything Science  Everyth...\n",
       "506    aplusguide A+ Guide A+ Guide is a place for po...\n",
       "66     Neurodivergent LGBTQ Neurodivergent LGBTQ Neur...\n",
       "293    The Owl House The Owl House A subreddit for th...\n",
       "317    traaaaaaannnnnnnnnns If youre memes and im mem...\n",
       "529    Savant Savant This subreddit is dedicated to t...\n",
       "278    Debate Vaccines Debate Vaccines Debate and dis...\n",
       "51     intj analytical, conceptual and objective For ...\n",
       "216    gaspies A place for gay aspies For the aspies ...\n",
       "219    Council Of Autism Council Of Autism This is no...\n",
       "190    BPD Borderline Personality Disorder  BPD is a ...\n",
       "207    wallstreetbets wallstreetbets Like 4chan found...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove \\n, /r/, r/, \\r  - these characters are not relevant\n",
    "asd_subs_df['text'] = asd_subs_df['text'].str.replace('\\n', ' ').str.replace('/r/', ' ').str.replace('r/', ' ').str.replace('\\r ', ' ')\n",
    "asd_subs_df['text'].sample(n=20, random_state=7) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to replace non-word characters with space - these chars are not relevant to the text analysis\n",
    "def remove_non_word_chars(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205    autismmemes autism memes Meme for people with ...\n",
       "488    ASDcareers The Careers of People with ASD Livi...\n",
       "231    Gamingcirclejerk Gaming Circlejerk   Don Chead...\n",
       "104    autisticpeople Autism Spectrum Disorder News  ...\n",
       "263                   Autistic Pride2 Autistic Pride2   \n",
       "492    Baby Bumps Baby Bumps A place for pregnant red...\n",
       "236    hypersensitivity Hypersensitivity A place for ...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "323    Everything Science Everything Science  Everyth...\n",
       "506    aplusguide A  Guide A  Guide is a place for po...\n",
       "66     Neurodivergent LGBTQ Neurodivergent LGBTQ Neur...\n",
       "293    The Owl House The Owl House A subreddit for th...\n",
       "317    traaaaaaannnnnnnnnns If youre memes and im mem...\n",
       "529    Savant Savant This subreddit is dedicated to t...\n",
       "278    Debate Vaccines Debate Vaccines Debate and dis...\n",
       "51     intj analytical  conceptual and objective For ...\n",
       "216    gaspies A place for gay aspies For the aspies ...\n",
       "219    Council Of Autism Council Of Autism This is no...\n",
       "190    BPD Borderline Personality Disorder  BPD is a ...\n",
       "207    wallstreetbets wallstreetbets Like 4chan found...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace non-word characters with space\n",
    "asd_subs_df['text'] = asd_subs_df['text'].apply(remove_non_word_chars)\n",
    "asd_subs_df['text'].sample(n=20, random_state=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to remove all extra white spaces:\n",
    "def remove_extra_white_spaces(text):\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205    autismmemes autism memes Meme for people with ...\n",
       "488    ASDcareers The Careers of People with ASD Livi...\n",
       "231    Gamingcirclejerk Gaming Circlejerk Don Cheadle...\n",
       "104    autisticpeople Autism Spectrum Disorder News l...\n",
       "263                     Autistic Pride2 Autistic Pride2 \n",
       "492    Baby Bumps Baby Bumps A place for pregnant red...\n",
       "236    hypersensitivity Hypersensitivity A place for ...\n",
       "3      nevergrewup When the body got older but the mi...\n",
       "323    Everything Science Everything Science Everythi...\n",
       "506    aplusguide A Guide A Guide is a place for poss...\n",
       "66     Neurodivergent LGBTQ Neurodivergent LGBTQ Neur...\n",
       "293    The Owl House The Owl House A subreddit for th...\n",
       "317    traaaaaaannnnnnnnnns If youre memes and im mem...\n",
       "529    Savant Savant This subreddit is dedicated to t...\n",
       "278    Debate Vaccines Debate Vaccines Debate and dis...\n",
       "51     intj analytical conceptual and objective For t...\n",
       "216    gaspies A place for gay aspies For the aspies ...\n",
       "219    Council Of Autism Council Of Autism This is no...\n",
       "190    BPD Borderline Personality Disorder BPD is a c...\n",
       "207    wallstreetbets wallstreetbets Like 4chan found...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove extra white spaces\n",
    "asd_subs_df['text'] = asd_subs_df['text'].apply(remove_extra_white_spaces)\n",
    "asd_subs_df['text'].sample(n=20, random_state=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 526 entries, 0 to 533\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   id            526 non-null    object\n",
      " 1   display_name  526 non-null    object\n",
      " 2   text          526 non-null    object\n",
      " 3   lang          526 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 20.5+ KB\n"
     ]
    }
   ],
   "source": [
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text will not be lowercased, as sometimes , for example, the acronym ASD or Asd or asd can appear in different contexts and i judge it to be important that all these instances are kept in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Get dataset for model selection -  anotated data\n",
    "40% of 526 = 210.4 -> split train-test 80%-20%( use cross validation as the dataset is small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Autism Representation Autism Awareness A place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aspie Positive Aspie Positive by aspies for as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trump Autism Trump Autism Trump Autism Trump A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Autism Representation Autism Awareness A place...\n",
       "1  Aspie Positive Aspie Positive by aspies for as...\n",
       "2  Trump Autism Trump Autism Trump Autism Trump A..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_annotated_df = asd_subs_df[['text']]\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 210 entries, 205 to 282\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    210 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# select at random 40% of rows in the dataset to be manually annotated into 2 categories: asd and other\n",
    "# 40% out of 542 = 210 - > text clasiffication needs a large number of documents to train : at least 100 , ideally 1000 ( http://benschmidt.org/medhist16/index.html%3Fp=16.html#:~:text=Topic%20modeling%20can%20work%20on,short%20as%20a%20paragraph%20apiece.)\\\n",
    "# it can be done with this dataset but it very close to the minumm limit ( limitation )\n",
    "np.random.seed(7)\n",
    "asd_subs_annotated_df = asd_subs_annotated_df.sample(frac=0.4)\n",
    "asd_subs_annotated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data in a csv file to be annotated\n",
    "asd_subs_annotated_df.to_csv('annotated_data/asd_subs_to_annotate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 210 entries, 0 to 209\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  210 non-null    int64 \n",
      " 1   text        210 non-null    object\n",
      " 2   Category    210 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# load annotated dataset\n",
    "data_path = 'annotated_data/asd_subs_annotated.csv'\n",
    "asd_subs_annotated_df = pd.read_csv(data_path)\n",
    "asd_subs_annotated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>205</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>488</td>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>231</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text Category\n",
       "0         205  autismmemes autism memes Meme for people with ...      asd\n",
       "1         488  ASDcareers The Careers of People with ASD Livi...      asd\n",
       "2         231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text Category\n",
       "Unnamed: 0                                                            \n",
       "205         autismmemes autism memes Meme for people with ...      asd\n",
       "488         ASDcareers The Careers of People with ASD Livi...      asd\n",
       "231         Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unnamed: 0 is the index in the original dataset\n",
    "# this column was changed by apple's Numbers program when the file was annotated\n",
    "# set it as the index ( to keep the original indexes of the subreddits )\n",
    "asd_subs_annotated_df.set_index('Unnamed: 0', inplace=True)\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Category\n",
       "205  autismmemes autism memes Meme for people with ...      asd\n",
       "488  ASDcareers The Careers of People with ASD Livi...      asd\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the index column name\n",
    "asd_subs_annotated_df.index.names = [None]\n",
    "# category contains 2 classes : asd (=autism related) and other (= not autism ralated)\n",
    "# the annotations were made by the author alone\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Category\n",
       "205  autismmemes autism memes Meme for people with ...      asd\n",
       "488  ASDcareers The Careers of People with ASD Livi...      asd\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# category contains 2 classes : asd (=autism related) and other (= not autism ralated)\n",
    "# the annotations were made by the author alone\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Asd</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>gaspies A place for gay aspies For the aspies ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Baby Bumps Baby Bumps A place for pregnant red...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asd</th>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohter</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ask Me Anything IAnswer We will try to answer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text                                                               \n",
       "         count unique                                                top freq\n",
       "Category                                                                     \n",
       "Asd          6      6  gaspies A place for gay aspies For the aspies ...    1\n",
       "Other        4      4  Baby Bumps Baby Bumps A place for pregnant red...    1\n",
       "asd        107    107  autismmemes autism memes Meme for people with ...    1\n",
       "ohter        1      1  Ask Me Anything IAnswer We will try to answer ...    1\n",
       "other       92     92  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# descriptive statistics\n",
    "asd_subs_annotated_df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asd</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohter</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Ask Me Anything IAnswer We will try to answer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text                                                               \n",
       "         count unique                                                top freq\n",
       "Category                                                                     \n",
       "asd        113    113  autismmemes autism memes Meme for people with ...    1\n",
       "ohter        1      1  Ask Me Anything IAnswer We will try to answer ...    1\n",
       "other       96     96  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lower case all entries in category column\n",
    "asd_subs_annotated_df['Category'] = asd_subs_annotated_df['Category'].str.lower()\n",
    "asd_subs_annotated_df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asd</th>\n",
       "      <td>113</td>\n",
       "      <td>113</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text                                                               \n",
       "         count unique                                                top freq\n",
       "Category                                                                     \n",
       "asd        113    113  autismmemes autism memes Meme for people with ...    1\n",
       "other       97     97  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one entry is miss spelled \n",
    "# replace 'ohter' for 'other' in Category\n",
    "asd_subs_annotated_df['Category'] = asd_subs_annotated_df['Category'].replace('ohter', 'other')\n",
    "asd_subs_annotated_df.groupby('Category').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is relatively ballanced, with 113 subreddits in the ASD category and 97 in the other (non-ASD) category. \n",
    "\n",
    "In balanced datasets accuracy can be a reasonable measure for assessing the performance of a model. However , other metrics will be used in addition to accuracy, to evaluate the performance of the model: precission, recall and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "      <td>1105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Category  text_length\n",
       "205  autismmemes autism memes Meme for people with ...      asd           71\n",
       "488  ASDcareers The Careers of People with ASD Livi...      asd         1105\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other         2073"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the length of text in a new column 'text_tength' \n",
    "asd_subs_annotated_df['text_length'] = asd_subs_annotated_df['text'].apply(lambda x: len(x))\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     210.000000\n",
      "mean     1732.747619\n",
      "std      2057.506036\n",
      "min        22.000000\n",
      "25%       200.250000\n",
      "50%       643.500000\n",
      "75%      2801.250000\n",
      "max      9506.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# text length statisitic for the whole dataset\n",
    "print(asd_subs_annotated_df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram for 'text_length' values\n",
    "plt.hist(asd_subs_annotated_df['text_length'], bins=10, edgecolor='k')\n",
    "\n",
    "# labels and title\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths Whole Dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text lenght statistics for Category=other\n",
      "count      97.000000\n",
      "mean     3035.422680\n",
      "std      2244.188401\n",
      "min        22.000000\n",
      "25%       928.000000\n",
      "50%      2657.000000\n",
      "75%      4755.000000\n",
      "max      9506.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "text lenght statistics for Category=asd\n",
      "count     113.000000\n",
      "mean      614.522124\n",
      "std       922.776421\n",
      "min        26.000000\n",
      "25%       117.000000\n",
      "50%       312.000000\n",
      "75%       610.000000\n",
      "max      4631.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# show text length for each of the 2 categories\n",
    "# filter rows where 'Category' = 'other'\n",
    "other_df = asd_subs_annotated_df[asd_subs_annotated_df['Category'] == 'other']\n",
    "# filter rows where 'Category' = 'asd'\n",
    "asd_df = asd_subs_annotated_df[asd_subs_annotated_df['Category'] == 'asd']\n",
    "\n",
    "# text lenght statistics for 'Category' = 'other'\n",
    "print('text lenght statistics for Category=other')\n",
    "print(other_df['text_length'].describe())\n",
    "# text lenght statistics for 'Category' = 'asd'\n",
    "print()\n",
    "print('text lenght statistics for Category=asd')\n",
    "print(asd_df['text_length'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram for 'text_length' values of other category\n",
    "plt.hist(other_df['text_length'], bins=10, edgecolor='k')\n",
    "\n",
    "# labels and title\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths for Other Category')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a histogram for 'text_length' values of asd category\n",
    "plt.hist(asd_df['text_length'], bins=10, edgecolor='k')\n",
    "\n",
    "# labels and title\n",
    "plt.xlabel('Text Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Text Lengths for ASD Category')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average text length for the whole data set is 1892 characters. Text descriptions vary between a minumum of 22 characters to a maximum of 9506 characters. \n",
    "\n",
    "The mean (1732) is almost triple the median value (643). This indicates that the distribution is positively skewed - the tail on the right side (higher values) is longer compared to the left side (lower values), as seen in the histogram above. This indicates that there are some very high values (outliers) pulling the mean upward. \n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/:\n",
    "Since our data is positively skewed here, it means that it has a higher number of data points having low values (shorter text descriptions). So when we train our model on this data, it will perform better at predicting the category of subreddtis with shorter descriptions as compared to those with longer descriptions. (?)\n",
    "\n",
    "This statistics represent text_lenght in terms of characters (inlcuding white-space). Skewed data is typical in NLP tasks. The text will need to be transformed (tokenisation) and projected into a feature space. \n",
    "All lenght text will be included in the analysis, in order to develop the largest vocabulary possible. \n",
    "As most text descriptions are under 1000 characters, i will expect models to perform best on short text.\n",
    "Also, the 'other' category text data seems to be more balancend, while the 'asd' category seems to containt mostly short text (under 500 chars). Thus i would expect the models to perform better at classifing the other category ( True  negavies). (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Category</th>\n",
       "      <th>text_length</th>\n",
       "      <th>asd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>asd</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>asd</td>\n",
       "      <td>1105</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>other</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text Category  text_length  \\\n",
       "205  autismmemes autism memes Meme for people with ...      asd           71   \n",
       "488  ASDcareers The Careers of People with ASD Livi...      asd         1105   \n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...    other         2073   \n",
       "\n",
       "     asd  \n",
       "205    1  \n",
       "488    1  \n",
       "231    0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Category is a categorical variable\n",
    "# needs to be turned into a numerical type for the analysis\n",
    "# create a new column 'Asd' for the numerical values of Category: 0 = Other, 1 = Asd\n",
    "asd_subs_annotated_df['asd']= asd_subs_annotated_df['Category'].apply(lambda x: 1 if x=='asd' else 0)\n",
    "asd_subs_annotated_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 210 entries, 205 to 282\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   text         210 non-null    object\n",
      " 1   Category     210 non-null    object\n",
      " 2   text_length  210 non-null    int64 \n",
      " 3   asd          210 non-null    int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 8.2+ KB\n"
     ]
    }
   ],
   "source": [
    "asd_subs_annotated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asd</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1</td>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asd                                               text\n",
       "205    1  autismmemes autism memes Meme for people with ...\n",
       "488    1  ASDcareers The Careers of People with ASD Livi...\n",
       "231    0  Gamingcirclejerk Gaming Circlejerk Don Cheadle..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will only use in the following analysis columns text and asd\n",
    "asd_subs_annotated_df_short = asd_subs_annotated_df[['asd', 'text']]\n",
    "asd_subs_annotated_df_short.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train set ( 0.8 of the dataset ) and test set of (0.2 of the dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = asd_subs_annotated_df_short[['text']]\n",
    "y = asd_subs_annotated_df_short[['asd']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 168 entries, 116 to 441\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    168 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.6+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 168 entries, 116 to 441\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   asd     168 non-null    int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 2.6 KB\n",
      "\n",
      "Asd-Other distribution\n",
      "asd\n",
      "1      90\n",
      "0      78\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Training Dataset')\n",
    "X_train.info()\n",
    "print()\n",
    "y_train.info()\n",
    "print()\n",
    "print('Asd-Other distribution')\n",
    "print(y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data contains 168 non-null values, and is relatively ballanced (90 subreddits are in the asd category and 78 are in the other category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42 entries, 231 to 426\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    42 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 672.0+ bytes\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 42 entries, 231 to 426\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   asd     42 non-null     int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 672.0 bytes\n",
      "\n",
      "Asd-Other distribution\n",
      "asd\n",
      "1      23\n",
      "0      19\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Test Dataset')\n",
    "X_test.info()\n",
    "print()\n",
    "y_test.info()\n",
    "print()\n",
    "print('Asd-Other distribution')\n",
    "print(y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data contains 42 non-null values, and is relatively ballanced (23 subreddits are in the asd category and 19 are in the other category)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both training and testing datasets are cleaned and ready for analysis.\n",
    "Cross validation will be used , instead of a stand alone validation set, due to the small training set size.\n",
    "\n",
    "I will apply different models used for supervised text classification. The best perfomant model will be used further to select the relevant asd(=autism) subreddits from the entire original dataset. \n",
    "\n",
    "List of models used:\n",
    "- text classification with Naive Bayes \n",
    "- text classification with KNeighborsClassifier\n",
    "- text classification with Random Forest\n",
    "- text classification with SVM (lienar = assumes the shape of the decision boundary is linear;  and RBF = for when you don't have prior knowledge about the shape of the decision boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Shallow learning \n",
    "\n",
    "Shallow learning models were, up until recently, the go-to approach for text classification. In terms of actual classification algorithms, these methods mostly rely on general purpose classifiers that are not specific to this context (NLP). The particular challenges presented by textual data ( examples: languare is ambiguous - ex same words have different meanings, long term dependencies, meaning depends on context; high variablity in grammar, spelligns etc; high dimensionality; data is very noisy - has misspellings and inconsistencies; data is unstructured etc.) are somewhat offloaded to the preceding steps of the TC pipeline (Preprocessing and Projection into features space = tokenization and vectorization), which consist in the extraction of machine-interpretable features and representations from documents (i.e., text interpretation).\n",
    "([Gasparetto et al., 2022, p. 10](zotero://select/library/items/7QI33EDP)) ([pdf](zotero://open-pdf/library/items/GJM85X9Y?page=10&annotation=9B4KWS9B))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-processing:\n",
    "Text data is unstructured data. Preprocessing this data is a first step to give this data some form of structure. \n",
    "\n",
    "    3.1 split text into tokens/words (=sentence segmentation/ tokanisation NLTK or Spcy or AutoTokenizer if using transformers)\n",
    "\n",
    "    3.3 stemming (=remove prefixes and sufixes) and lemmatization (=get the base word - ext ate becomes (to)eat) - in this project lemmatization will be used as it can more accurately find the base word ( it has more complex rules than stemming -  see https://www.analyticsvidhya.com/blog/2022/06/stemming-vs-lemmatization-in-nlp-must-know-differences/#:~:text=Stemming%20is%20a%20process%20that,form%2C%20which%20is%20called%20Lemma.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>womenonthespectrum womenonthespectrum A commun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>The Owl House The Owl House A subreddit for th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Autism Speaks Sucks Autism Speaks Sucks A comm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "116  womenonthespectrum womenonthespectrum A commun...\n",
       "293  The Owl House The Owl House A subreddit for th...\n",
       "452  Autism Speaks Sucks Autism Speaks Sucks A comm..."
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any stop words ( that were potentially missed by regex above) - > not relevant to the classification task\n",
    "# remove punctuation - not relevant to classification (?maybe)\n",
    "# lemmatize tokens\n",
    "def preprocess(text):\n",
    "    doc= nlp(text)\n",
    "    new_text = []\n",
    "    for t in doc:\n",
    "        if t.is_stop or t.is_punct:\n",
    "            continue\n",
    "        new_text.append(t.lemma_)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>womenonthespectrum womenonthespectrum A commun...</td>\n",
       "      <td>womenonthespectrum womenonthespectrum communit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>The Owl House The Owl House A subreddit for th...</td>\n",
       "      <td>Owl House Owl House subreddit Disney fantasy c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Autism Speaks Sucks Autism Speaks Sucks A comm...</td>\n",
       "      <td>autism speak Sucks autism speak suck community...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "116  womenonthespectrum womenonthespectrum A commun...   \n",
       "293  The Owl House The Owl House A subreddit for th...   \n",
       "452  Autism Speaks Sucks Autism Speaks Sucks A comm...   \n",
       "\n",
       "                                              new_text  \n",
       "116  womenonthespectrum womenonthespectrum communit...  \n",
       "293  Owl House Owl House subreddit Disney fantasy c...  \n",
       "452  autism speak Sucks autism speak suck community...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process traning set \n",
    "X_train.loc[:, 'new_text'] = X_train['text'].apply(preprocess)\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['womenonthespectrum', 'womenonthespectrum', 'A', 'community', 'specifically', 'for', 'women', 'nonbinary', 'and', 'afab', 'people', 'on', 'the', 'spectrum', 'to', 'vent', 'discuss', 'topics', 'related', 'to', 'autism', 'and', 'support', 'each', 'other']\n",
      "['womenonthespectrum', 'womenonthespectrum', 'community', 'specifically', 'woman', 'nonbinary', 'afab', 'people', 'spectrum', 'vent', 'discuss', 'topic', 'relate', 'autism', 'support']\n"
     ]
    }
   ],
   "source": [
    "# difference in text before and after preprocessing\n",
    "doc_text = nlp(X_train.text[116])\n",
    "doc_new_text = nlp(X_train.new_text[116])\n",
    "print([t.text for t in doc_text])\n",
    "print([t.text for t in doc_new_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>Gamingcirclejerk gaming Circlejerk Don Cheadle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>ASDpeersupport ASD Peer Support Peer based sup...</td>\n",
       "      <td>ASDpeersupport ASD Peer support Peer base supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>selfharm A Subreddit for Self Harmers A subred...</td>\n",
       "      <td>selfharm Subreddit Self Harmers subreddit self...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...   \n",
       "243  ASDpeersupport ASD Peer Support Peer based sup...   \n",
       "392  selfharm A Subreddit for Self Harmers A subred...   \n",
       "\n",
       "                                              new_text  \n",
       "231  Gamingcirclejerk gaming Circlejerk Don Cheadle...  \n",
       "243  ASDpeersupport ASD Peer support Peer base supp...  \n",
       "392  selfharm Subreddit Self Harmers subreddit self...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-process testinf set \n",
    "X_test.loc[:, 'new_text'] = X_test['text'].apply(preprocess)\n",
    "X_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 5. 6. Vectorize, Apply Classifier and Evaluate the model - uses sklearn.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4. feature engineering (= convert text / document into vector)\n",
    "    count-vector\n",
    "    tf-idf\n",
    "    one-hot encode - not really used due to size and sparcity problems\n",
    "    word/token-emebeding\n",
    "5. apply classifier ( any machine learning model classifier ) - use gridSearchCV to see which is the best performing classifier ?\n",
    "6. evaluate  model: Accuracy, Precision, Recall, F1 score ( if not good , go back to preprocessing step )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 1. Vectorization does not involve any machine learning and is only based on applying frequency counts to tokens/words (Count Vectorization and TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def classify(vectorizer, model, x_train, y_train, x_test, y_test, k_folds=5, scaler=None):\n",
    "    # create the pipeline\n",
    "    clf = Pipeline([\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('scaler', scaler),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    # initialize StratifiedKFold for cross-validation\n",
    "    kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # perform k-fold (k=5) cross-validation \n",
    "    y_pred_cv = cross_val_predict(clf, x_train, y_train, cv=kf)\n",
    "    \n",
    "    # fit the model on the training data\n",
    "    clf.fit(x_train, y_train)\n",
    "    # make prediction using test data\n",
    "    y_pred = clf.predict(x_test)\n",
    "\n",
    "    # classification report and confusion matrix for cross-validated predictions\n",
    "    print(\"Cross-validated Classification Report:\\n\", classification_report(y_train, y_pred_cv))\n",
    "    print(\"Cross-validated Confusion Matrix:\\n\", confusion_matrix(y_train, y_pred_cv))\n",
    "    \n",
    "    # classification report and confusion matrix for test data \n",
    "    print(\"\\nTest Data Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Test Data Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168,), (168,))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.asd.shape, X_train.new_text.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n",
      "CountVectorizer() KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.51      0.67        78\n",
      "           1       0.70      0.98      0.81        90\n",
      "\n",
      "    accuracy                           0.76       168\n",
      "   macro avg       0.83      0.75      0.74       168\n",
      "weighted avg       0.82      0.76      0.75       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[40 38]\n",
      " [ 2 88]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.53      0.69        19\n",
      "           1       0.72      1.00      0.84        23\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.86      0.76      0.76        42\n",
      "weighted avg       0.85      0.79      0.77        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[10  9]\n",
      " [ 0 23]]\n",
      "CountVectorizer() MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91        78\n",
      "           1       0.91      0.96      0.93        90\n",
      "\n",
      "    accuracy                           0.92       168\n",
      "   macro avg       0.93      0.92      0.92       168\n",
      "weighted avg       0.92      0.92      0.92       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[69  9]\n",
      " [ 4 86]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        19\n",
      "           1       0.85      0.96      0.90        23\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.89      0.87      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 1 22]]\n",
      "CountVectorizer() RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        78\n",
      "           1       0.82      0.93      0.87        90\n",
      "\n",
      "    accuracy                           0.86       168\n",
      "   macro avg       0.87      0.85      0.85       168\n",
      "weighted avg       0.86      0.86      0.86       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[60 18]\n",
      " [ 6 84]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.71        19\n",
      "           1       0.74      0.87      0.80        23\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.77      0.75      0.75        42\n",
      "weighted avg       0.77      0.76      0.76        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 3 20]]\n",
      "CountVectorizer() SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.78      0.84        78\n",
      "           1       0.83      0.93      0.88        90\n",
      "\n",
      "    accuracy                           0.86       168\n",
      "   macro avg       0.87      0.86      0.86       168\n",
      "weighted avg       0.87      0.86      0.86       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[61 17]\n",
      " [ 6 84]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.63      0.77        19\n",
      "           1       0.77      1.00      0.87        23\n",
      "\n",
      "    accuracy                           0.83        42\n",
      "   macro avg       0.88      0.82      0.82        42\n",
      "weighted avg       0.87      0.83      0.83        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 0 23]]\n",
      "CountVectorizer() SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.67      0.78        78\n",
      "           1       0.77      0.96      0.85        90\n",
      "\n",
      "    accuracy                           0.82       168\n",
      "   macro avg       0.85      0.81      0.81       168\n",
      "weighted avg       0.84      0.82      0.82       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[52 26]\n",
      " [ 4 86]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75        19\n",
      "           1       0.76      0.96      0.85        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.84      0.79      0.80        42\n",
      "weighted avg       0.83      0.81      0.80        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 1 22]]\n",
      "\n",
      "TfidfVectorizer()\n",
      "TfidfVectorizer() KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88        78\n",
      "           1       0.87      0.96      0.91        90\n",
      "\n",
      "    accuracy                           0.90       168\n",
      "   macro avg       0.91      0.89      0.90       168\n",
      "weighted avg       0.90      0.90      0.90       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[65 13]\n",
      " [ 4 86]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        19\n",
      "           1       0.85      0.96      0.90        23\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.89      0.87      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 1 22]]\n",
      "TfidfVectorizer() MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91        78\n",
      "           1       0.89      0.97      0.93        90\n",
      "\n",
      "    accuracy                           0.92       168\n",
      "   macro avg       0.92      0.91      0.92       168\n",
      "weighted avg       0.92      0.92      0.92       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[67 11]\n",
      " [ 3 87]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        19\n",
      "           1       0.85      0.96      0.90        23\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.89      0.87      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 1 22]]\n",
      "TfidfVectorizer() RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.77      0.84        78\n",
      "           1       0.83      0.94      0.88        90\n",
      "\n",
      "    accuracy                           0.86       168\n",
      "   macro avg       0.87      0.86      0.86       168\n",
      "weighted avg       0.87      0.86      0.86       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[60 18]\n",
      " [ 5 85]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76        19\n",
      "           1       0.78      0.91      0.84        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.82      0.80      0.80        42\n",
      "weighted avg       0.82      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 2 21]]\n",
      "TfidfVectorizer() SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94        78\n",
      "           1       0.94      0.97      0.95        90\n",
      "\n",
      "    accuracy                           0.95       168\n",
      "   macro avg       0.95      0.94      0.95       168\n",
      "weighted avg       0.95      0.95      0.95       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[72  6]\n",
      " [ 3 87]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91        19\n",
      "           1       0.88      1.00      0.94        23\n",
      "\n",
      "    accuracy                           0.93        42\n",
      "   macro avg       0.94      0.92      0.93        42\n",
      "weighted avg       0.94      0.93      0.93        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[16  3]\n",
      " [ 0 23]]\n",
      "TfidfVectorizer() SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.87      0.91        78\n",
      "           1       0.90      0.97      0.93        90\n",
      "\n",
      "    accuracy                           0.92       168\n",
      "   macro avg       0.93      0.92      0.92       168\n",
      "weighted avg       0.93      0.92      0.92       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[68 10]\n",
      " [ 3 87]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        19\n",
      "           1       0.85      1.00      0.92        23\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.93      0.89      0.90        42\n",
      "weighted avg       0.92      0.90      0.90        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 0 23]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "vectorizers = [CountVectorizer(), TfidfVectorizer()]\n",
    "models = [KNeighborsClassifier(), MultinomialNB(), RandomForestClassifier(), SVC(kernel='linear'), SVC(kernel='rbf')]\n",
    "\n",
    "for v in vectorizers:\n",
    "    print(v)\n",
    "    for m in models:\n",
    "        print(v, m)\n",
    "        classify(v, m, X_train.new_text, y_train.asd, X_test.new_text, y_test.asd)\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the models classify non-ASD subreddits (0) with higher precission - as expected (see above discussion ).\n",
    "\n",
    "Using a simple frequency word count vectorisation method (CountVectorizer) resulted in an accuracy on test data of:\n",
    "\n",
    "CountVectorizer: \n",
    "\n",
    "0.79 - for KNN (trainig data 0.76 => not overfit)<br>\n",
    "0.88 - for multinomial naive bayes (training data 0.92 => maybe overfit?)<br>\n",
    "0.76 - for the Random Forest (training data 0.86 => overift)<br>\n",
    "0.83 - for SVM linear kernel (training data 0.86=> maybe overfit?)<br>\n",
    "0.81 - for SVM rbf kernel (training data 0.82 => not overfit)<br>\n",
    "\n",
    "TF-IDF:\n",
    "\n",
    "0.88 - for the KNN (training data 0.90 => not overfit)<br>\n",
    "0.88 - for multinomial naive bayes (training data 0.92 => maybe overfit?)<br>\n",
    "0.81 - for the Random Forest (training data 0.86 => maybe overift)<br>\n",
    "0.93 - for SVM linear kernel (training data 0.95 => not overfit)<br>\n",
    "0.90 - for SVM rbf kernel (training data 0.92 => not overfit)<br>\n",
    "\n",
    "The predictions were improved, as expected, when using TF-IDF to vectorize the tokens ( this way frequent words, common to all the documents are given lower importance, and less common words are given more weight). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 2. Vectorization using shallow neural nets (spaCy's GloVe)\n",
    "Token vectors are the result of utilising a shallow neural networks in order to create a high dimensional vector for each word (though they are much smaller and denser than, for example, TF-IDF vectors). - GloVe ( why not word2vec? - easier to train as it creates lower dimentions vectors; more suitable for large datasets or limited computer power; neither is able to handle OOV - out of vocab -  words). GloVe's unit on which the neural nets is trained is *word*.  \n",
    "\n",
    "maybe try also FastText?\n",
    "FastText is able to handle OOV words, as long as it has seen its composing n-grams during training. Both GloVe and Word2Vec are instead unable to handle the case of OOV words. ([Gasparetto et al., 2022, p. 10](zotero://select/library/items/7QI33EDP)) ([pdf](zotero://open-pdf/library/items/GJM85X9Y?page=10&annotation=6H5SYFFG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# spaCy large English model provides vectors (GloVe)\n",
    "# all vectors are of lenght 300 (= 300 dimensions )\n",
    "for text in X_train.text:\n",
    "    doc = nlp(text)\n",
    "    print(doc.vector.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy 's English language model - large , provides 514k keys, 514k unique vectors (each=300 dimensions) (https://spacy.io/models/en)\n",
    "\n",
    "This model provides GloVe vectors trained on Common Crawl (https://github.com/explosion/spacy-models/releases//tag/en_core_web_lg-2.3.1) \n",
    "\n",
    "Common Crawl database: https://en.wikipedia.org/wiki/Common_Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8486128e+00  1.5892310e+00 -3.2409065e+00 -4.3019190e-02\n",
      "  4.0609274e+00  4.7510800e-01  4.8715359e-01  4.1747169e+00\n",
      " -1.4294057e+00  4.5436048e-03  5.3522205e+00  2.2574072e+00\n",
      " -4.5851417e+00  1.3712009e+00  4.4433114e-01  2.2298615e+00\n",
      "  2.1136811e+00 -1.3794917e+00 -3.0008421e+00 -2.5509632e+00\n",
      "  8.7831706e-02 -1.9485286e+00 -1.1433463e+00 -2.1321458e-01\n",
      "  1.0417217e+00 -1.7193288e+00 -2.3502305e+00  9.7079217e-01\n",
      " -2.0635519e+00  6.4479995e-01  8.1923783e-01 -4.6718398e-01\n",
      " -1.9332402e+00 -3.0946743e+00 -1.9094207e+00 -5.7582241e-01\n",
      "  6.8277502e-01  3.5950679e-01  9.2482871e-01  2.8120521e-01\n",
      "  1.7119380e+00 -9.0557039e-02  1.2482457e-01 -8.7007560e-02\n",
      " -1.6891894e+00  1.6687484e+00  9.7832167e-01 -1.8785739e+00\n",
      " -1.3737924e+00  2.2778971e+00 -4.9244021e-03  2.4940062e+00\n",
      " -1.3629605e+00 -4.8081613e+00 -1.8509319e+00  1.8132876e+00\n",
      " -1.2980604e+00  1.1212356e+00  1.4739300e+00 -1.1941451e+00\n",
      "  9.5256023e-02  4.5365158e-01 -1.1192365e+00 -3.8787171e-02\n",
      "  2.7721593e+00  1.7451249e+00 -2.3981118e+00 -4.4940038e+00\n",
      "  9.5901239e-01  2.2615461e+00 -1.1881301e+00 -7.1540523e-01\n",
      " -2.5427940e+00  2.1812581e-01 -1.2869862e+00  1.8550901e+00\n",
      " -2.7567263e+00  2.1776268e+00 -3.3992243e+00 -8.2173623e-02\n",
      " -4.4456820e+00 -1.1336416e+00  7.3921371e-01  1.4418510e+00\n",
      "  2.0911708e+00  8.6766618e-01 -2.0605171e+00 -2.5663702e+00\n",
      "  1.1602172e+00 -9.6260804e-01 -6.4950472e-01  4.4791320e-01\n",
      "  1.0453191e-01 -3.2109497e+00  4.9352124e-01 -8.1007409e-01\n",
      "  1.1597320e+00 -1.1953099e+00  1.2387642e+00  2.2561636e+00\n",
      "  2.2526960e+00  8.8909179e-01  2.8272388e+00  2.1895928e+00\n",
      "  7.9014070e-02  4.4063263e+00 -1.5271601e-02 -3.5424263e+00\n",
      " -7.9467827e-01 -4.1521716e+00  2.8720887e+00  6.7872858e-01\n",
      " -9.9017352e-01  6.7907405e-01  1.2675071e+00  1.0349966e+00\n",
      " -6.4931160e-01 -1.2466155e+00  7.8239441e-01 -2.3689656e+00\n",
      " -2.6846299e+00 -2.4371803e+00  6.2432599e-01 -7.7566838e-01\n",
      " -3.1817323e-01 -3.8693261e+00  8.1504995e-01 -2.2502677e+00\n",
      "  2.5226393e+00 -1.5871588e+00 -3.4664960e+00 -5.9366941e-01\n",
      "  4.4501424e+00  1.0376472e+00  7.2079635e-01  1.7279395e+00\n",
      " -2.4075055e+00  5.9464639e-01  2.9307523e+00 -3.1697249e-01\n",
      " -1.8443201e+00 -8.4204674e-01  4.0485736e-02  1.4430989e+00\n",
      "  1.3717177e+00  2.8942195e-01 -2.1683633e+00  9.6444952e-01\n",
      "  9.1153848e-01  1.6861616e+00  2.7284345e-01  3.1786628e+00\n",
      " -2.6884973e-01  2.2603402e-01 -1.2949392e+00  6.5053278e-01\n",
      "  2.6353955e+00  4.1818479e-01 -1.8691132e+00 -1.6162257e+00\n",
      " -1.8289201e+00 -1.3636334e+00  1.6683270e-01  3.2047284e+00\n",
      " -1.6804321e+00 -1.6570185e+00 -4.5860090e+00  5.6198752e-01\n",
      "  1.2184116e+00 -1.5503386e+00  6.4979398e-01 -1.1733575e+00\n",
      "  1.8647968e+00  1.7883879e+00  1.6589230e+00  8.5050356e-01\n",
      "  5.2783036e-01 -1.0516793e+00 -3.7319841e+00 -3.3036938e+00\n",
      " -3.2144952e-01 -1.0420176e+00  1.2993311e+00 -2.4131296e+00\n",
      " -1.4800589e+00  9.8974299e-01 -2.2334754e+00 -6.7935264e-01\n",
      "  1.8875611e+00  1.9104831e+00 -2.5119683e-01 -1.5109971e+00\n",
      " -3.5590798e-01 -8.7727880e-01  5.2809083e-01  1.2623860e+00\n",
      " -3.0134747e+00  1.3460220e-01 -5.1972222e-01  4.6624955e-02\n",
      " -2.3708181e+00 -2.2317383e+00 -1.6532526e+00 -1.9728584e+00\n",
      "  3.8928397e+00  4.4453561e-01 -3.8113313e+00  1.3557708e+00\n",
      "  2.0109360e-01 -1.7748319e+00  1.4592683e+00  3.4003600e-01\n",
      " -9.6155703e-01  1.0240515e+00  5.9323645e-01  2.6453490e+00\n",
      "  9.5779228e-01 -2.7430100e+00 -9.8531753e-01  6.9066793e-02\n",
      " -2.1379478e+00  2.5216544e+00  3.8593638e-01 -6.7103988e-01\n",
      " -6.8593049e-01 -1.7129228e+00 -7.4767166e-01  1.8136367e-01\n",
      "  1.7403755e+00  7.5642252e-01  1.1847917e+00 -2.3679345e+00\n",
      " -1.6187577e-01 -1.6355841e-01  3.9168487e+00  1.0257910e+00\n",
      " -6.5368307e-01  9.3705767e-01 -4.2148936e-01 -1.8597522e-01\n",
      " -8.8167989e-01 -7.6361322e-01  1.7101768e+00  3.6566058e-01\n",
      " -5.4800361e-01  2.4975631e+00 -2.8282723e+00 -3.2815036e-01\n",
      "  2.4966881e-01  1.8636829e+00 -3.5992801e-01 -2.5831344e+00\n",
      " -2.8959243e+00 -8.0204874e-01  5.1763439e-01 -2.3424032e+00\n",
      "  3.0093272e+00 -4.1533554e-01  1.7821886e-01  1.4374208e+00\n",
      "  4.3918204e-01  5.5658002e+00  3.0900249e+00  3.4415536e+00\n",
      "  2.9662037e+00 -1.4320481e+00 -7.6286745e-01  1.8285425e+00\n",
      " -1.8780159e+00  1.4745520e-01 -5.6848764e-01 -1.3089867e+00\n",
      "  6.6980195e-01 -2.8438430e+00  1.1433554e+00 -8.8800544e-01\n",
      "  1.4381065e+00 -9.1180146e-01 -2.2206919e+00  7.1590751e-01\n",
      "  1.9339584e+00 -4.8441201e-02  1.1920552e+00  1.8568896e+00\n",
      "  1.7066920e-01 -1.8847406e-01 -1.3079999e-01  1.4817383e+00\n",
      " -1.1341300e+00 -6.7267245e-01 -7.7272844e-01 -4.9119607e-02\n",
      " -1.0566320e+00  1.5772431e-01 -5.5967039e-01 -5.8379763e-01\n",
      "  8.2700950e-01 -2.0801191e+00 -3.4544075e+00  9.8871636e-01]\n"
     ]
    }
   ],
   "source": [
    "for text in X_train.text:\n",
    "    doc = nlp(text)\n",
    "    print(doc.vector)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a vectorizer function that uses spaCy GloVe vectors\n",
    "def text_to_vectors(texts):\n",
    "    return [nlp(text).vector for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.92      0.83        78\n",
      "           1       0.92      0.73      0.81        90\n",
      "\n",
      "    accuracy                           0.82       168\n",
      "   macro avg       0.83      0.83      0.82       168\n",
      "weighted avg       0.84      0.82      0.82       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[72  6]\n",
      " [24 66]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.76        19\n",
      "           1       0.84      0.70      0.76        23\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.77      0.77      0.76        42\n",
      "weighted avg       0.78      0.76      0.76        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[16  3]\n",
      " [ 7 16]]\n",
      "\n",
      "GloVe MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86        78\n",
      "           1       0.89      0.86      0.87        90\n",
      "\n",
      "    accuracy                           0.86       168\n",
      "   macro avg       0.86      0.86      0.86       168\n",
      "weighted avg       0.86      0.86      0.86       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[68 10]\n",
      " [13 77]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        19\n",
      "           1       0.83      0.83      0.83        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.81      0.81      0.81        42\n",
      "weighted avg       0.81      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 4 19]]\n",
      "\n",
      "GloVe RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.83      0.86        78\n",
      "           1       0.86      0.90      0.88        90\n",
      "\n",
      "    accuracy                           0.87       168\n",
      "   macro avg       0.87      0.87      0.87       168\n",
      "weighted avg       0.87      0.87      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[65 13]\n",
      " [ 9 81]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81        19\n",
      "           1       0.79      1.00      0.88        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.90      0.84      0.85        42\n",
      "weighted avg       0.89      0.86      0.85        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 0 23]]\n",
      "\n",
      "GloVe SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.83      0.83        78\n",
      "           1       0.86      0.86      0.86        90\n",
      "\n",
      "    accuracy                           0.85       168\n",
      "   macro avg       0.84      0.84      0.84       168\n",
      "weighted avg       0.85      0.85      0.85       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[65 13]\n",
      " [13 77]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78        19\n",
      "           1       0.80      0.87      0.83        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.81      0.80      0.81        42\n",
      "weighted avg       0.81      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[14  5]\n",
      " [ 3 20]]\n",
      "\n",
      "GloVe SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86        78\n",
      "           1       0.89      0.87      0.88        90\n",
      "\n",
      "    accuracy                           0.87       168\n",
      "   macro avg       0.87      0.87      0.87       168\n",
      "weighted avg       0.87      0.87      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[68 10]\n",
      " [12 78]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.74      0.78        19\n",
      "           1       0.80      0.87      0.83        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.81      0.80      0.81        42\n",
      "weighted avg       0.81      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[14  5]\n",
      " [ 3 20]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#GloVe vectors contain negative values and this is not suitable to MultinomialNB()  ( it needs positive values to work )\n",
    "# we can rescale with MinMaxScaler = This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\n",
    "# for different types and applications of Naive Bayse : https://www.ibm.com/topics/naive-bayes#:~:text=The%20Na%C3%AFve%20Bayes%20classifier%20is,a%20given%20class%20or%20category.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# save the vectorizer function above into a format suitable for use in the clf pipeline ( in the classify function )\n",
    "vect = FunctionTransformer(text_to_vectors)\n",
    "\n",
    "# fit the models and \n",
    "for m in models:\n",
    "    print('GloVe', m)\n",
    "\n",
    "    if isinstance(m, MultinomialNB):\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    classify(vect, m, X_train.new_text, y_train.asd, X_test.new_text, y_test.asd, scaler=scaler)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GloVe embedings, some models classified ASD subreddits (1) with higher or equal precission, compared to the Other category (0) - contrary to what was expected ( see above discussion ) - why? maybe ASD class has shorter text, but more consistent patterns.\n",
    "\n",
    "Using a word embeding method method (GloVe) resulted in an accuracy ranging from:\n",
    "\n",
    "0.76 - for KNN ( training data was 0.82 => overfit)<br>\n",
    "0.81 - multinomial naive bayes (training data 0.86 => maybe overfit)<br>\n",
    "0.86 - for Random Forest ( training data 0.87 => not overfit)<br>\n",
    "0.81 - SVM linear kernel (training data 0.85 => not overfit)<br>\n",
    "0.81 - SVM RBF kernel ( training data 0.87 => maybe overfit?)<br>\n",
    "\n",
    "\n",
    "The performance of the model using GloVe embedings is lower that vectorization using count frequencies (see above - accuracy range (0.76 to 0.93 )). This might be due to the information loss resulted after GloVe ( - it does dimensionality reduction ). Another reason could be the difference in type of text between the current data base and Common Crawl ( on which the GloVe embedings were trained ) - CountVectorizer and TF-IDF create own specific word vectors for the current dataset, while spaCy's GloVe model uses pretrained ones (we used out of the box, pretrained vectors ). \n",
    "\n",
    "GloVe models utilise a dimensionality reduction step in order to handle the large dimensions of the word co-occurrence matrix that it uses in its calculations. Although compressing representations can arguably lead to a more robust representation (as it theoretically forces the model to try to preserve the most significant pieces of information), a bigger advantage comes from the fact that this approach is more suitable for parallelisation, making it easier to train on more data. ([Gasparetto et al., 2022, p. 9](zotero://select/library/items/7QI33EDP)) ([pdf](zotero://open-pdf/library/items/GJM85X9Y?page=9&annotation=7IIF5HKM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 3. Vectorization using shallow neural nets (word2vec in Gensim)\n",
    "\n",
    "SpaCy's out of the box GloVe model does not perform very well on domain specific data. \n",
    "Apply word2vec -> use gensim as it makes it easier to train own model and has more out of the box models to choose from.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 3a. word2vec out of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300',\n",
      " 'conceptnet-numberbatch-17-06-300',\n",
      " 'word2vec-ruscorpora-300',\n",
      " 'word2vec-google-news-300',\n",
      " 'glove-wiki-gigaword-50',\n",
      " 'glove-wiki-gigaword-100',\n",
      " 'glove-wiki-gigaword-200',\n",
      " 'glove-wiki-gigaword-300',\n",
      " 'glove-twitter-25',\n",
      " 'glove-twitter-50',\n",
      " 'glove-twitter-100',\n",
      " 'glove-twitter-200',\n",
      " '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from pprint import pprint\n",
    "\n",
    "# list pretrained models available in gensim\n",
    "# the training copus used for available models for english is google-news\n",
    "# this type of text is different then text on reddit \n",
    "# the expectation is that the out of the box model will not have improved performance compared to count vectorizers used initially\n",
    "pprint(list(api.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the word2vec-google-news-300 pretrained model - the vector size is 300\n",
    "embeding_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('autism_spectrum_disorders', 0.8032151460647583),\n",
       " ('autistic', 0.7907201051712036),\n",
       " ('Autism', 0.7736356854438782),\n",
       " ('autism_spectrum_disorder', 0.7603526711463928),\n",
       " ('autism_spectrum', 0.7037034630775452),\n",
       " ('ASDs', 0.6698784232139587),\n",
       " ('developmental_disorders', 0.6663666367530823),\n",
       " ('ADHD', 0.6658616662025452),\n",
       " ('Autism_Spectrum_Disorder_ASD', 0.6601078510284424),\n",
       " ('developmental_disorder', 0.6575655937194824)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeding_model.most_similar('autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>womenonthespectrum womenonthespectrum A commun...</td>\n",
       "      <td>womenonthespectrum womenonthespectrum communit...</td>\n",
       "      <td>[womenonthespectrum, womenonthespectrum, commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>The Owl House The Owl House A subreddit for th...</td>\n",
       "      <td>Owl House Owl House subreddit Disney fantasy c...</td>\n",
       "      <td>[Owl, House, Owl, House, subreddit, Disney, fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "116  womenonthespectrum womenonthespectrum A commun...   \n",
       "293  The Owl House The Owl House A subreddit for th...   \n",
       "\n",
       "                                              new_text  \\\n",
       "116  womenonthespectrum womenonthespectrum communit...   \n",
       "293  Owl House Owl House subreddit Disney fantasy c...   \n",
       "\n",
       "                                        tokenized_text  \n",
       "116  [womenonthespectrum, womenonthespectrum, commu...  \n",
       "293  [Owl, House, Owl, House, subreddit, Disney, fa...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize training text\n",
    "X_train['tokenized_text'] = X_train.new_text.apply(gensim.utils.tokenize)\n",
    "X_train['tokenized_text'] = X_train.tokenized_text.apply(list)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>Gamingcirclejerk gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>[Gamingcirclejerk, gaming, Circlejerk, Don, Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>ASDpeersupport ASD Peer Support Peer based sup...</td>\n",
       "      <td>ASDpeersupport ASD Peer support Peer base supp...</td>\n",
       "      <td>[ASDpeersupport, ASD, Peer, support, Peer, bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...   \n",
       "243  ASDpeersupport ASD Peer Support Peer based sup...   \n",
       "\n",
       "                                              new_text  \\\n",
       "231  Gamingcirclejerk gaming Circlejerk Don Cheadle...   \n",
       "243  ASDpeersupport ASD Peer support Peer base supp...   \n",
       "\n",
       "                                        tokenized_text  \n",
       "231  [Gamingcirclejerk, gaming, Circlejerk, Don, Ch...  \n",
       "243  [ASDpeersupport, ASD, Peer, support, Peer, bas...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize testing text\n",
    "X_test['tokenized_text'] = X_test.new_text.apply(gensim.utils.tokenize)\n",
    "X_test['tokenized_text'] = X_test.tokenized_text.apply(list)\n",
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the text vectors \n",
    "def pretrained_vectorizer(X, vector_size=300):\n",
    "    doc_vectors = []\n",
    "    for index, row in X.iterrows():\n",
    "        tokenized_text = row['tokenized_text'] \n",
    "        doc_vector = np.zeros(vector_size)\n",
    "        for word in tokenized_text:\n",
    "            if word in embeding_model:\n",
    "                doc_vector += embeding_model[word]\n",
    "\n",
    "        doc_vectors.append(doc_vector)\n",
    "\n",
    "    return np.array(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 300\n",
      "685 300\n",
      "14 300\n",
      "230 300\n",
      "12 300\n",
      "7 300\n",
      "40 300\n",
      "352 300\n",
      "177 300\n",
      "8 300\n",
      "844 300\n",
      "490 300\n",
      "4 300\n",
      "350 300\n",
      "315 300\n",
      "527 300\n",
      "69 300\n",
      "99 300\n",
      "166 300\n",
      "33 300\n",
      "36 300\n",
      "547 300\n",
      "12 300\n",
      "4 300\n",
      "152 300\n",
      "50 300\n",
      "72 300\n",
      "377 300\n",
      "4 300\n",
      "544 300\n",
      "492 300\n",
      "60 300\n",
      "465 300\n",
      "40 300\n",
      "4 300\n",
      "232 300\n",
      "4 300\n",
      "12 300\n",
      "10 300\n",
      "11 300\n",
      "853 300\n",
      "13 300\n",
      "453 300\n",
      "81 300\n",
      "122 300\n",
      "786 300\n",
      "153 300\n",
      "226 300\n",
      "376 300\n",
      "27 300\n",
      "35 300\n",
      "549 300\n",
      "79 300\n",
      "36 300\n",
      "60 300\n",
      "137 300\n",
      "10 300\n",
      "539 300\n",
      "28 300\n",
      "48 300\n",
      "23 300\n",
      "26 300\n",
      "47 300\n",
      "51 300\n",
      "141 300\n",
      "4 300\n",
      "114 300\n",
      "130 300\n",
      "166 300\n",
      "40 300\n",
      "480 300\n",
      "466 300\n",
      "28 300\n",
      "11 300\n",
      "660 300\n",
      "663 300\n",
      "338 300\n",
      "16 300\n",
      "31 300\n",
      "320 300\n",
      "12 300\n",
      "99 300\n",
      "234 300\n",
      "20 300\n",
      "87 300\n",
      "37 300\n",
      "113 300\n",
      "12 300\n",
      "42 300\n",
      "11 300\n",
      "162 300\n",
      "44 300\n",
      "631 300\n",
      "50 300\n",
      "270 300\n",
      "12 300\n",
      "73 300\n",
      "45 300\n",
      "12 300\n",
      "9 300\n",
      "14 300\n",
      "179 300\n",
      "269 300\n",
      "9 300\n",
      "521 300\n",
      "15 300\n",
      "674 300\n",
      "60 300\n",
      "595 300\n",
      "468 300\n",
      "386 300\n",
      "75 300\n",
      "4 300\n",
      "169 300\n",
      "38 300\n",
      "50 300\n",
      "35 300\n",
      "12 300\n",
      "18 300\n",
      "9 300\n",
      "418 300\n",
      "13 300\n",
      "21 300\n",
      "18 300\n",
      "330 300\n",
      "18 300\n",
      "459 300\n",
      "846 300\n",
      "656 300\n",
      "64 300\n",
      "295 300\n",
      "56 300\n",
      "13 300\n",
      "35 300\n",
      "9 300\n",
      "55 300\n",
      "8 300\n",
      "421 300\n",
      "35 300\n",
      "64 300\n",
      "305 300\n",
      "451 300\n",
      "464 300\n",
      "543 300\n",
      "10 300\n",
      "10 300\n",
      "77 300\n",
      "22 300\n",
      "215 300\n",
      "9 300\n",
      "12 300\n",
      "78 300\n",
      "604 300\n",
      "23 300\n",
      "26 300\n",
      "14 300\n",
      "60 300\n",
      "949 300\n",
      "148 300\n",
      "487 300\n",
      "9 300\n",
      "492 300\n",
      "67 300\n",
      "88 300\n",
      "56 300\n",
      "96 300\n",
      "350 300\n",
      "72 300\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = pretrained_vectorizer(X_train)\n",
    "# all the text vectors are the same size \n",
    "for i, v in enumerate(X_train_vectorized):\n",
    "    print(len(X_train.tokenized_text.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2Vec-pretrained KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.74      0.82        78\n",
      "           1       0.81      0.93      0.87        90\n",
      "\n",
      "    accuracy                           0.85       168\n",
      "   macro avg       0.86      0.84      0.84       168\n",
      "weighted avg       0.85      0.85      0.84       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[58 20]\n",
      " [ 6 84]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.63      0.75        19\n",
      "           1       0.76      0.96      0.85        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.84      0.79      0.80        42\n",
      "weighted avg       0.83      0.81      0.80        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 1 22]]\n",
      "\n",
      "word2Vec-pretrained MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.63      0.74        78\n",
      "           1       0.75      0.94      0.83        90\n",
      "\n",
      "    accuracy                           0.80       168\n",
      "   macro avg       0.83      0.79      0.79       168\n",
      "weighted avg       0.82      0.80      0.79       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[49 29]\n",
      " [ 5 85]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.71        19\n",
      "           1       0.74      0.87      0.80        23\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.77      0.75      0.75        42\n",
      "weighted avg       0.77      0.76      0.76        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 3 20]]\n",
      "\n",
      "word2Vec-pretrained RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        78\n",
      "           1       0.87      0.89      0.88        90\n",
      "\n",
      "    accuracy                           0.87       168\n",
      "   macro avg       0.87      0.87      0.87       168\n",
      "weighted avg       0.87      0.87      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[66 12]\n",
      " [10 80]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.74      0.72        19\n",
      "           1       0.77      0.74      0.76        23\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.74      0.74      0.74        42\n",
      "weighted avg       0.74      0.74      0.74        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[14  5]\n",
      " [ 6 17]]\n",
      "\n",
      "word2Vec-pretrained SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87        78\n",
      "           1       0.86      0.94      0.90        90\n",
      "\n",
      "    accuracy                           0.89       168\n",
      "   macro avg       0.89      0.88      0.89       168\n",
      "weighted avg       0.89      0.89      0.89       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[64 14]\n",
      " [ 5 85]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.68      0.76        19\n",
      "           1       0.78      0.91      0.84        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.82      0.80      0.80        42\n",
      "weighted avg       0.82      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 2 21]]\n",
      "\n",
      "word2Vec-pretrained SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.76        78\n",
      "           1       0.76      0.93      0.84        90\n",
      "\n",
      "    accuracy                           0.81       168\n",
      "   macro avg       0.83      0.80      0.80       168\n",
      "weighted avg       0.83      0.81      0.81       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[52 26]\n",
      " [ 6 84]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74        19\n",
      "           1       0.77      0.87      0.82        23\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.79      0.78      0.78        42\n",
      "weighted avg       0.79      0.79      0.78        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 3 20]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "# save the vectorizer function above into a format suitable for use in the clf pipeline ( in the classify function )\n",
    "vect_pretrained = FunctionTransformer(pretrained_vectorizer, validate=False)\n",
    "\n",
    "# fit the classification models \n",
    "for m in models:\n",
    "    print('word2Vec-pretrained', m)\n",
    "\n",
    "    if isinstance(m, MultinomialNB):\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    classify(vect_pretrained, m, X_train, y_train.asd, X_test, y_test.asd, scaler=scaler)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With word2vec embedings, most models classified the Other category (0) with higher  precission - as was expected ( see above discussion ).\n",
    "\n",
    "Using a word embeding method method (word2Vec - google news corpus - from Gensim) resulted in an accuracy ranging from:\n",
    "\n",
    "0.81 - for KNN ( training data was 0.85 => not overfit)<br>\n",
    "0.76 - multinomial naive bayes (training data 0.80 => overfit)<br>\n",
    "0.74 - for Random Forest ( training data 0.87 => overfit)<br>\n",
    "0.81 - SVM linear kernel (training data 0.89 => maybe overfit)<br>\n",
    "0.79 - SVM RBF kernel ( training data 0.81 => not overfit)<br>\n",
    "\n",
    "\n",
    "Same as GloVe, the performance of the models using word2vec embedings is lower than vectorization using count frequencies (see above - accuracy range (0.76 to 0.93 )). The same reasoning applies: the difference in type of text between the current data base and google news text ( on which the word2vec embedings were trained ) - CountVectorizer and TF-IDF create own specific word vectors for the current dataset, while word2vec model uses pretrained ones (we used out of the box, pretrained vectors ). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 3b. word2vec trained on the present corpus \n",
    "The dataset is small, and this approach is expected to result in poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rare-technologies.com/word2vec-tutorial/\n",
    "# initialise a word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    vector_size=300,\n",
    "    window=10,\n",
    "    min_count=5,     # some text descriptions are very short and we want to include all the information/data in the training\n",
    "    epochs=10       # default is 5\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a vocab\n",
    "np.random.seed(7)\n",
    "w2v_model.build_vocab(X_train.tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176658, 310770)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "np.random.seed(7)\n",
    "w2v_model.train(\n",
    "    X_train.tokenized_text, \n",
    "    total_examples=w2v_model.corpus_count,\n",
    "    epochs=w2v_model.epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('models/w2v_subreddits.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Autism', 0.9980832934379578),\n",
       " ('spectrum', 0.9933792948722839),\n",
       " ('Asperger', 0.9885281324386597),\n",
       " ('discuss', 0.9876161813735962),\n",
       " ('community', 0.9857353568077087),\n",
       " ('advice', 0.9855016469955444),\n",
       " ('place', 0.9844009876251221),\n",
       " ('relate', 0.9840799570083618),\n",
       " ('help', 0.9830957055091858),\n",
       " ('clinically', 0.9830114245414734)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-9d8cfb01f8ee>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
      "<ipython-input-88-9d8cfb01f8ee>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "# get word vectors\n",
    "words = set(w2v_model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train.tokenized_text])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test.tokenized_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 12\n",
      "685 198\n",
      "14 13\n",
      "230 140\n",
      "12 11\n",
      "7 6\n",
      "40 30\n",
      "352 254\n",
      "177 121\n",
      "8 7\n",
      "844 622\n",
      "490 308\n",
      "4 4\n",
      "350 280\n",
      "315 205\n",
      "527 437\n",
      "69 59\n",
      "99 62\n",
      "166 132\n",
      "33 19\n",
      "36 23\n",
      "547 380\n",
      "12 9\n",
      "4 2\n",
      "152 102\n",
      "50 43\n",
      "72 46\n",
      "377 286\n",
      "4 3\n",
      "544 478\n",
      "492 344\n",
      "60 35\n",
      "465 359\n",
      "40 30\n",
      "4 2\n",
      "232 151\n",
      "4 2\n",
      "12 6\n",
      "10 7\n",
      "11 10\n",
      "853 635\n",
      "13 7\n",
      "453 309\n",
      "81 63\n",
      "122 75\n",
      "786 607\n",
      "153 112\n",
      "226 184\n",
      "376 281\n",
      "27 21\n",
      "35 31\n",
      "549 409\n",
      "79 42\n",
      "36 21\n",
      "60 54\n",
      "137 95\n",
      "10 10\n",
      "539 371\n",
      "28 18\n",
      "48 37\n",
      "23 17\n",
      "26 20\n",
      "47 41\n",
      "51 48\n",
      "141 124\n",
      "4 4\n",
      "114 62\n",
      "130 102\n",
      "166 132\n",
      "40 28\n",
      "480 414\n",
      "466 340\n",
      "28 20\n",
      "11 9\n",
      "660 510\n",
      "663 479\n",
      "338 235\n",
      "16 12\n",
      "31 21\n",
      "320 247\n",
      "12 4\n",
      "99 89\n",
      "234 152\n",
      "20 10\n",
      "87 57\n",
      "37 30\n",
      "113 76\n",
      "12 10\n",
      "42 26\n",
      "11 7\n",
      "162 116\n",
      "44 28\n",
      "631 516\n",
      "50 39\n",
      "270 207\n",
      "12 10\n",
      "73 41\n",
      "45 39\n",
      "12 11\n",
      "9 6\n",
      "14 14\n",
      "179 132\n",
      "269 205\n",
      "9 3\n",
      "521 386\n",
      "15 15\n",
      "674 519\n",
      "60 52\n",
      "595 456\n",
      "468 222\n",
      "386 281\n",
      "75 21\n",
      "4 0\n",
      "169 135\n",
      "38 26\n",
      "50 28\n",
      "35 27\n",
      "12 8\n",
      "18 7\n",
      "9 4\n",
      "418 298\n",
      "13 11\n",
      "21 12\n",
      "18 13\n",
      "330 228\n",
      "18 16\n",
      "459 363\n",
      "846 753\n",
      "656 455\n",
      "64 52\n",
      "295 229\n",
      "56 46\n",
      "13 12\n",
      "35 14\n",
      "9 8\n",
      "55 50\n",
      "8 4\n",
      "421 313\n",
      "35 24\n",
      "64 53\n",
      "305 232\n",
      "451 314\n",
      "464 274\n",
      "543 403\n",
      "10 6\n",
      "10 10\n",
      "77 61\n",
      "22 18\n",
      "215 180\n",
      "9 5\n",
      "12 10\n",
      "78 68\n",
      "604 407\n",
      "23 11\n",
      "26 16\n",
      "14 10\n",
      "60 54\n",
      "949 609\n",
      "148 105\n",
      "487 402\n",
      "9 4\n",
      "492 412\n",
      "67 46\n",
      "88 72\n",
      "56 20\n",
      "96 79\n",
      "350 272\n",
      "72 48\n"
     ]
    }
   ],
   "source": [
    "# the vectors obtained above are not of equal size for the whole corpus\n",
    "# we need to average the word vectors into a text vector of size 300 - each text will be represented as a vector of size 300 \n",
    "# size = 300 because this it the size we used so far for the other models and for consistenci reasons we kept the same for the current model\n",
    "for i, v in enumerate(X_train_vect):\n",
    "    print(len(X_train.tokenized_text.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((168,), (42,))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect.shape, X_test_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute text vectors by averaging the word vectors for the words contained in the text\n",
    "# average word vector to get a singel text vector of size=300 ( see above discussion )\n",
    "def w2v_vectorizer(X, vector_size=300):\n",
    "    words = set(w2v_model.wv.index_to_key)\n",
    "    X_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                      for ls in X])\n",
    "\n",
    "    X_vect_avg = []\n",
    "    for v in X_vect:\n",
    "        if v.size:\n",
    "            X_vect_avg.append(v.mean(axis=0))\n",
    "        else:\n",
    "            X_vect_avg.append(np.zeros(vector_size, dtype=float))\n",
    "\n",
    "    return np.array(X_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 300\n",
      "685 300\n",
      "14 300\n",
      "230 300\n",
      "12 300\n",
      "7 300\n",
      "40 300\n",
      "352 300\n",
      "177 300\n",
      "8 300\n",
      "844 300\n",
      "490 300\n",
      "4 300\n",
      "350 300\n",
      "315 300\n",
      "527 300\n",
      "69 300\n",
      "99 300\n",
      "166 300\n",
      "33 300\n",
      "36 300\n",
      "547 300\n",
      "12 300\n",
      "4 300\n",
      "152 300\n",
      "50 300\n",
      "72 300\n",
      "377 300\n",
      "4 300\n",
      "544 300\n",
      "492 300\n",
      "60 300\n",
      "465 300\n",
      "40 300\n",
      "4 300\n",
      "232 300\n",
      "4 300\n",
      "12 300\n",
      "10 300\n",
      "11 300\n",
      "853 300\n",
      "13 300\n",
      "453 300\n",
      "81 300\n",
      "122 300\n",
      "786 300\n",
      "153 300\n",
      "226 300\n",
      "376 300\n",
      "27 300\n",
      "35 300\n",
      "549 300\n",
      "79 300\n",
      "36 300\n",
      "60 300\n",
      "137 300\n",
      "10 300\n",
      "539 300\n",
      "28 300\n",
      "48 300\n",
      "23 300\n",
      "26 300\n",
      "47 300\n",
      "51 300\n",
      "141 300\n",
      "4 300\n",
      "114 300\n",
      "130 300\n",
      "166 300\n",
      "40 300\n",
      "480 300\n",
      "466 300\n",
      "28 300\n",
      "11 300\n",
      "660 300\n",
      "663 300\n",
      "338 300\n",
      "16 300\n",
      "31 300\n",
      "320 300\n",
      "12 300\n",
      "99 300\n",
      "234 300\n",
      "20 300\n",
      "87 300\n",
      "37 300\n",
      "113 300\n",
      "12 300\n",
      "42 300\n",
      "11 300\n",
      "162 300\n",
      "44 300\n",
      "631 300\n",
      "50 300\n",
      "270 300\n",
      "12 300\n",
      "73 300\n",
      "45 300\n",
      "12 300\n",
      "9 300\n",
      "14 300\n",
      "179 300\n",
      "269 300\n",
      "9 300\n",
      "521 300\n",
      "15 300\n",
      "674 300\n",
      "60 300\n",
      "595 300\n",
      "468 300\n",
      "386 300\n",
      "75 300\n",
      "4 300\n",
      "169 300\n",
      "38 300\n",
      "50 300\n",
      "35 300\n",
      "12 300\n",
      "18 300\n",
      "9 300\n",
      "418 300\n",
      "13 300\n",
      "21 300\n",
      "18 300\n",
      "330 300\n",
      "18 300\n",
      "459 300\n",
      "846 300\n",
      "656 300\n",
      "64 300\n",
      "295 300\n",
      "56 300\n",
      "13 300\n",
      "35 300\n",
      "9 300\n",
      "55 300\n",
      "8 300\n",
      "421 300\n",
      "35 300\n",
      "64 300\n",
      "305 300\n",
      "451 300\n",
      "464 300\n",
      "543 300\n",
      "10 300\n",
      "10 300\n",
      "77 300\n",
      "22 300\n",
      "215 300\n",
      "9 300\n",
      "12 300\n",
      "78 300\n",
      "604 300\n",
      "23 300\n",
      "26 300\n",
      "14 300\n",
      "60 300\n",
      "949 300\n",
      "148 300\n",
      "487 300\n",
      "9 300\n",
      "492 300\n",
      "67 300\n",
      "88 300\n",
      "56 300\n",
      "96 300\n",
      "350 300\n",
      "72 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-91-4c27ac376226>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = w2v_vectorizer(X_train.tokenized_text)\n",
    "# now all the text vectors are the same size\n",
    "for i, v in enumerate(X_train_vectorized):\n",
    "    print(len(X_train.tokenized_text.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[KNeighborsClassifier(),\n",
       " MultinomialNB(),\n",
       " RandomForestClassifier(),\n",
       " SVC(kernel='linear'),\n",
       " SVC()]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a model for predictions - this also evaluates the performance of current word2vec model\n",
    "# (word2vec is an unsupervised model; thus it can only be evaluated when solving a specific task - here text classification)\n",
    "# classification models used ( to be consistent with the previous analysis )\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom trained word2Vec KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        78\n",
      "           1       0.91      0.88      0.89        90\n",
      "\n",
      "    accuracy                           0.89       168\n",
      "   macro avg       0.89      0.89      0.89       168\n",
      "weighted avg       0.89      0.89      0.89       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[70  8]\n",
      " [11 79]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83        19\n",
      "           1       0.84      0.91      0.87        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.86      0.85      0.85        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 2 21]]\n",
      "\n",
      "Custom trained word2Vec MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.76      0.82        78\n",
      "           1       0.81      0.92      0.86        90\n",
      "\n",
      "    accuracy                           0.85       168\n",
      "   macro avg       0.85      0.84      0.84       168\n",
      "weighted avg       0.85      0.85      0.84       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[59 19]\n",
      " [ 7 83]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81        19\n",
      "           1       0.79      1.00      0.88        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.90      0.84      0.85        42\n",
      "weighted avg       0.89      0.86      0.85        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 0 23]]\n",
      "\n",
      "Custom trained word2Vec RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86        78\n",
      "           1       0.88      0.89      0.88        90\n",
      "\n",
      "    accuracy                           0.88       168\n",
      "   macro avg       0.87      0.87      0.87       168\n",
      "weighted avg       0.87      0.88      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[67 11]\n",
      " [10 80]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.79      0.86        19\n",
      "           1       0.85      0.96      0.90        23\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.89      0.87      0.88        42\n",
      "weighted avg       0.89      0.88      0.88        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 1 22]]\n",
      "\n",
      "Custom trained word2Vec SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88        78\n",
      "           1       0.91      0.88      0.89        90\n",
      "\n",
      "    accuracy                           0.89       168\n",
      "   macro avg       0.89      0.89      0.89       168\n",
      "weighted avg       0.89      0.89      0.89       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[70  8]\n",
      " [11 79]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85        19\n",
      "           1       0.82      1.00      0.90        23\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.91      0.87      0.88        42\n",
      "weighted avg       0.90      0.88      0.88        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[14  5]\n",
      " [ 0 23]]\n",
      "\n",
      "Custom trained word2Vec SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        78\n",
      "           1       0.87      0.89      0.88        90\n",
      "\n",
      "    accuracy                           0.87       168\n",
      "   macro avg       0.87      0.87      0.87       168\n",
      "weighted avg       0.87      0.87      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[66 12]\n",
      " [10 80]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81        19\n",
      "           1       0.79      1.00      0.88        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.90      0.84      0.85        42\n",
      "weighted avg       0.89      0.86      0.85        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 0 23]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# dont show VisibleDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# save the vectorizer function above into a format suitable for use in the clf pipeline ( in the classify function )\n",
    "vect = FunctionTransformer(w2v_vectorizer, validate=False)\n",
    "\n",
    "# fit the classification models \n",
    "for m in models:\n",
    "    print('Custom trained word2Vec', m)\n",
    "\n",
    "    if isinstance(m, MultinomialNB):\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    classify(vect, m, X_train.tokenized_text, y_train.asd, X_test.tokenized_text, y_test.asd, scaler=scaler)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec embedings trained on the current dataset, performed better than the out of the box, pretrained model available in Gensim. Accuracy for the models:\n",
    "\n",
    "0.88 - for KNN ( training data was 0.88 => not overfit)<br>\n",
    "0.86 - multinomial naive bayes (training data 0.83 => not overfit)<br>\n",
    "0.88 - for Random Forest ( training data 0.89 => not overfit)<br>\n",
    "0.88 - SVM linear kernel (training data 0.86 => not overfit)<br>\n",
    "0.86 - SVM RBF kernel ( training data 0.85 => not overfit)<br>\n",
    "\n",
    "Training a word2vec model on this current dataset did not improve performance when compared to previous frequency based vectorizers. This is a complex model, and for it to perform best it needs a much larger dataset than the one we currently have. This analysis indicates that perhaps for simple text classification tasks, with a small dataset, classical vectorization tehniques based on word counts might be more appropriate. \n",
    "\n",
    "However it is worth noting that the overall, performance on test sets was better than trainig, sugessting that the models trained with these vectors are less likely to be overfitted. Also we notice consistent, similar performance with all 4 classification models used - this is in line with the reasoning that in NLP the quality of text representations is more important that the actual machine learning algorithm used for classification or prediction (need citation). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 4. Vectorization using shallow neural nets (GloVe - pretrained on twitter data )\n",
    "\n",
    "To my knowledge there is no  pretrained model on Reddit data. But there are emebding models trained on social media data : for ex. glove-twitter-200 from the Gensim library. Will this result in a better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the glove-twitter-200 pretrained model - the vector size is 200\n",
    "embeding_model = api.load('glove-twitter-200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 200\n",
      "685 200\n",
      "14 200\n",
      "230 200\n",
      "12 200\n",
      "7 200\n",
      "40 200\n",
      "352 200\n",
      "177 200\n",
      "8 200\n",
      "844 200\n",
      "490 200\n",
      "4 200\n",
      "350 200\n",
      "315 200\n",
      "527 200\n",
      "69 200\n",
      "99 200\n",
      "166 200\n",
      "33 200\n",
      "36 200\n",
      "547 200\n",
      "12 200\n",
      "4 200\n",
      "152 200\n",
      "50 200\n",
      "72 200\n",
      "377 200\n",
      "4 200\n",
      "544 200\n",
      "492 200\n",
      "60 200\n",
      "465 200\n",
      "40 200\n",
      "4 200\n",
      "232 200\n",
      "4 200\n",
      "12 200\n",
      "10 200\n",
      "11 200\n",
      "853 200\n",
      "13 200\n",
      "453 200\n",
      "81 200\n",
      "122 200\n",
      "786 200\n",
      "153 200\n",
      "226 200\n",
      "376 200\n",
      "27 200\n",
      "35 200\n",
      "549 200\n",
      "79 200\n",
      "36 200\n",
      "60 200\n",
      "137 200\n",
      "10 200\n",
      "539 200\n",
      "28 200\n",
      "48 200\n",
      "23 200\n",
      "26 200\n",
      "47 200\n",
      "51 200\n",
      "141 200\n",
      "4 200\n",
      "114 200\n",
      "130 200\n",
      "166 200\n",
      "40 200\n",
      "480 200\n",
      "466 200\n",
      "28 200\n",
      "11 200\n",
      "660 200\n",
      "663 200\n",
      "338 200\n",
      "16 200\n",
      "31 200\n",
      "320 200\n",
      "12 200\n",
      "99 200\n",
      "234 200\n",
      "20 200\n",
      "87 200\n",
      "37 200\n",
      "113 200\n",
      "12 200\n",
      "42 200\n",
      "11 200\n",
      "162 200\n",
      "44 200\n",
      "631 200\n",
      "50 200\n",
      "270 200\n",
      "12 200\n",
      "73 200\n",
      "45 200\n",
      "12 200\n",
      "9 200\n",
      "14 200\n",
      "179 200\n",
      "269 200\n",
      "9 200\n",
      "521 200\n",
      "15 200\n",
      "674 200\n",
      "60 200\n",
      "595 200\n",
      "468 200\n",
      "386 200\n",
      "75 200\n",
      "4 200\n",
      "169 200\n",
      "38 200\n",
      "50 200\n",
      "35 200\n",
      "12 200\n",
      "18 200\n",
      "9 200\n",
      "418 200\n",
      "13 200\n",
      "21 200\n",
      "18 200\n",
      "330 200\n",
      "18 200\n",
      "459 200\n",
      "846 200\n",
      "656 200\n",
      "64 200\n",
      "295 200\n",
      "56 200\n",
      "13 200\n",
      "35 200\n",
      "9 200\n",
      "55 200\n",
      "8 200\n",
      "421 200\n",
      "35 200\n",
      "64 200\n",
      "305 200\n",
      "451 200\n",
      "464 200\n",
      "543 200\n",
      "10 200\n",
      "10 200\n",
      "77 200\n",
      "22 200\n",
      "215 200\n",
      "9 200\n",
      "12 200\n",
      "78 200\n",
      "604 200\n",
      "23 200\n",
      "26 200\n",
      "14 200\n",
      "60 200\n",
      "949 200\n",
      "148 200\n",
      "487 200\n",
      "9 200\n",
      "492 200\n",
      "67 200\n",
      "88 200\n",
      "56 200\n",
      "96 200\n",
      "350 200\n",
      "72 200\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = pretrained_vectorizer(X_train,vector_size=200)\n",
    "# all the text vectors are the same size \n",
    "for i, v in enumerate(X_train_vectorized):\n",
    "    print(len(X_train.tokenized_text.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer function for vector size=200\n",
    "def pretrained_vectorizer200(X):\n",
    "    return pretrained_vectorizer(X, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Twitter KNeighborsClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79        78\n",
      "           1       0.79      0.90      0.84        90\n",
      "\n",
      "    accuracy                           0.82       168\n",
      "   macro avg       0.83      0.82      0.82       168\n",
      "weighted avg       0.83      0.82      0.82       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[57 21]\n",
      " [ 9 81]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.58      0.65        19\n",
      "           1       0.70      0.83      0.76        23\n",
      "\n",
      "    accuracy                           0.71        42\n",
      "   macro avg       0.72      0.70      0.70        42\n",
      "weighted avg       0.72      0.71      0.71        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[11  8]\n",
      " [ 4 19]]\n",
      "\n",
      "GloVe Twitter MultinomialNB()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.60      0.72        78\n",
      "           1       0.73      0.94      0.83        90\n",
      "\n",
      "    accuracy                           0.79       168\n",
      "   macro avg       0.82      0.77      0.77       168\n",
      "weighted avg       0.81      0.79      0.78       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[47 31]\n",
      " [ 5 85]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.58      0.67        19\n",
      "           1       0.71      0.87      0.78        23\n",
      "\n",
      "    accuracy                           0.74        42\n",
      "   macro avg       0.75      0.72      0.73        42\n",
      "weighted avg       0.75      0.74      0.73        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[11  8]\n",
      " [ 3 20]]\n",
      "\n",
      "GloVe Twitter RandomForestClassifier()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85        78\n",
      "           1       0.85      0.90      0.88        90\n",
      "\n",
      "    accuracy                           0.86       168\n",
      "   macro avg       0.86      0.86      0.86       168\n",
      "weighted avg       0.86      0.86      0.86       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[64 14]\n",
      " [ 9 81]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        19\n",
      "           1       0.79      0.83      0.81        23\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.78      0.78      0.78        42\n",
      "weighted avg       0.79      0.79      0.79        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[14  5]\n",
      " [ 4 19]]\n",
      "\n",
      "GloVe Twitter SVC(kernel='linear')\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.74      0.76        78\n",
      "           1       0.79      0.82      0.80        90\n",
      "\n",
      "    accuracy                           0.79       168\n",
      "   macro avg       0.79      0.78      0.78       168\n",
      "weighted avg       0.79      0.79      0.79       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[58 20]\n",
      " [16 74]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.68      0.74        19\n",
      "           1       0.77      0.87      0.82        23\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.79      0.78      0.78        42\n",
      "weighted avg       0.79      0.79      0.78        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[13  6]\n",
      " [ 3 20]]\n",
      "\n",
      "GloVe Twitter SVC()\n",
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.68      0.77        78\n",
      "           1       0.77      0.92      0.84        90\n",
      "\n",
      "    accuracy                           0.81       168\n",
      "   macro avg       0.83      0.80      0.80       168\n",
      "weighted avg       0.82      0.81      0.81       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[53 25]\n",
      " [ 7 83]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.63      0.71        19\n",
      "           1       0.74      0.87      0.80        23\n",
      "\n",
      "    accuracy                           0.76        42\n",
      "   macro avg       0.77      0.75      0.75        42\n",
      "weighted avg       0.77      0.76      0.76        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[12  7]\n",
      " [ 3 20]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "# save the vectorizer function above into a format suitable for use in the clf pipeline ( in the classify function )\n",
    "vect_pretrained = FunctionTransformer(pretrained_vectorizer200, validate=False)\n",
    "\n",
    "# fit the classification models \n",
    "for m in models:\n",
    "    print('GloVe Twitter', m)\n",
    "\n",
    "    if isinstance(m, MultinomialNB):\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    classify(vect_pretrained, m, X_train, y_train.asd, X_test, y_test.asd, scaler=scaler)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GloVe model trained on twitter data did not improve perfomance. \n",
    "For the present dataset the frequency count vectorizers and the custom trained word2vec vectorizer seem to perform the best so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Approach 5. Vectorization using shallow neural nets (FastText)\n",
    "\n",
    "https://fasttext.cc/<br>\n",
    "\n",
    "why? Argued to result in better word representations and also deals with OOV problem + can be be easly trained to create vectors specific to the current dataset.\n",
    "\n",
    "FastText is one of the most novel techniques, developed by Bojanowski et al. [35]. The main concern addressed by this method is the fact that its predecessors ignore the morphology of words by assigning a distinct vector to each word. ([Gasparetto et al., 2022, p. 10](zotero://select/library/items/7QI33EDP)) ([pdf](zotero://open-pdf/library/items/GJM85X9Y?page=10&annotation=IZPGLGLF))\n",
    "\n",
    "FastText is able to handle OOV words, as long as it has seen its composing n-grams during training. Both GloVe and Word2Vec are instead unable to handle the case of OOV words. ([Gasparetto et al., 2022, p. 10](zotero://select/library/items/7QI33EDP)) ([pdf](zotero://open-pdf/library/items/GJM85X9Y?page=10&annotation=6H5SYFFG)) - FastText unit on which the neural net is trained is *character n-gram* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>womenonthespectrum womenonthespectrum A commun...</td>\n",
       "      <td>womenonthespectrum womenonthespectrum communit...</td>\n",
       "      <td>[womenonthespectrum, womenonthespectrum, commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>The Owl House The Owl House A subreddit for th...</td>\n",
       "      <td>Owl House Owl House subreddit Disney fantasy c...</td>\n",
       "      <td>[Owl, House, Owl, House, subreddit, Disney, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Autism Speaks Sucks Autism Speaks Sucks A comm...</td>\n",
       "      <td>autism speak Sucks autism speak suck community...</td>\n",
       "      <td>[autism, speak, Sucks, autism, speak, suck, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "116  womenonthespectrum womenonthespectrum A commun...   \n",
       "293  The Owl House The Owl House A subreddit for th...   \n",
       "452  Autism Speaks Sucks Autism Speaks Sucks A comm...   \n",
       "\n",
       "                                              new_text  \\\n",
       "116  womenonthespectrum womenonthespectrum communit...   \n",
       "293  Owl House Owl House subreddit Disney fantasy c...   \n",
       "452  autism speak Sucks autism speak suck community...   \n",
       "\n",
       "                                        tokenized_text  \n",
       "116  [womenonthespectrum, womenonthespectrum, commu...  \n",
       "293  [Owl, House, Owl, House, subreddit, Disney, fa...  \n",
       "452  [autism, speak, Sucks, autism, speak, suck, co...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116    womenonthespectrum womenonthespectrum communit...\n",
       "293    Owl House Owl House subreddit Disney fantasy c...\n",
       "452    autism speak Sucks autism speak suck community...\n",
       "Name: new_text, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will use the new_text (the clenaed/prerpocessed text)\n",
    "X_train.new_text.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Approach 5a. Pretrained FastText vectors + classical classification algorithms</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_path = r'models/fasttext_pretrained/crawl-300d-2M-subword/crawl-300d-2M-subword.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# load the pretrained english model fro fasttext - trained on Crawl dataset, vector size=300\n",
    "ft_en_model = fasttext.load_model(ft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8590065836906433, 'Autism'),\n",
       " (0.8022950887680054, 'autistic'),\n",
       " (0.7881932854652405, 'autisms'),\n",
       " (0.7856281399726868, 'autism-'),\n",
       " (0.7737045288085938, 'autism-related'),\n",
       " (0.7696868777275085, 'austism'),\n",
       " (0.766737163066864, 'non-autism'),\n",
       " (0.7601718902587891, 'ASD'),\n",
       " (0.7600443959236145, 'autistics'),\n",
       " (0.7465263605117798, 'autism-spectrum')]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_en_model.get_nearest_neighbors('autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a fastText vecotrizer \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FastTextVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, fasttext_model_path):\n",
    "        self.fasttext_model_path = fasttext_model_path\n",
    "        self.fasttext_model = None  # initialize the FastText model as None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # load the FastText model only once during fitting\n",
    "        if self.fasttext_model is None:\n",
    "            self.fasttext_model = fasttext.load_model(self.fasttext_model_path)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        # check model is loaded before transforming\n",
    "        if self.fasttext_model is None:\n",
    "            raise ValueError(\"FastText model is not loaded. Call fit() before transform().\")\n",
    "\n",
    "        vectors = [self.fasttext_model.get_sentence_vector(text) for text in X]\n",
    "        return np.array(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the fasttext vectorizer function above into a format suitable for use in the clf pipeline ( in the classify function )\n",
    "ft_vectorizer = FastTextVectorizer(ft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastText en-pretrained KNeighborsClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89        78\n",
      "           1       0.92      0.89      0.90        90\n",
      "\n",
      "    accuracy                           0.90       168\n",
      "   macro avg       0.90      0.90      0.90       168\n",
      "weighted avg       0.90      0.90      0.90       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[71  7]\n",
      " [10 80]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        19\n",
      "           1       0.83      0.83      0.83        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.81      0.81      0.81        42\n",
      "weighted avg       0.81      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 4 19]]\n",
      "\n",
      "fastText en-pretrained MultinomialNB()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87        78\n",
      "           1       0.95      0.80      0.87        90\n",
      "\n",
      "    accuracy                           0.87       168\n",
      "   macro avg       0.88      0.87      0.87       168\n",
      "weighted avg       0.88      0.87      0.87       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[74  4]\n",
      " [18 72]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.84      0.78        19\n",
      "           1       0.85      0.74      0.79        23\n",
      "\n",
      "    accuracy                           0.79        42\n",
      "   macro avg       0.79      0.79      0.79        42\n",
      "weighted avg       0.79      0.79      0.79        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[16  3]\n",
      " [ 6 17]]\n",
      "\n",
      "fastText en-pretrained RandomForestClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90        78\n",
      "           1       0.89      0.94      0.92        90\n",
      "\n",
      "    accuracy                           0.91       168\n",
      "   macro avg       0.91      0.91      0.91       168\n",
      "weighted avg       0.91      0.91      0.91       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[68 10]\n",
      " [ 5 85]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79        19\n",
      "           1       0.83      0.83      0.83        23\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.81      0.81      0.81        42\n",
      "weighted avg       0.81      0.81      0.81        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[15  4]\n",
      " [ 4 19]]\n",
      "\n",
      "fastText en-pretrained SVC(kernel='linear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89        78\n",
      "           1       0.91      0.89      0.90        90\n",
      "\n",
      "    accuracy                           0.89       168\n",
      "   macro avg       0.89      0.89      0.89       168\n",
      "weighted avg       0.89      0.89      0.89       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[70  8]\n",
      " [10 80]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.84      0.82        19\n",
      "           1       0.86      0.83      0.84        23\n",
      "\n",
      "    accuracy                           0.83        42\n",
      "   macro avg       0.83      0.83      0.83        42\n",
      "weighted avg       0.83      0.83      0.83        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[16  3]\n",
      " [ 4 19]]\n",
      "\n",
      "fastText en-pretrained SVC()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90        78\n",
      "           1       0.92      0.91      0.92        90\n",
      "\n",
      "    accuracy                           0.91       168\n",
      "   macro avg       0.91      0.91      0.91       168\n",
      "weighted avg       0.91      0.91      0.91       168\n",
      "\n",
      "Cross-validated Confusion Matrix:\n",
      " [[71  7]\n",
      " [ 8 82]]\n",
      "\n",
      "Test Data Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.84      0.84        19\n",
      "           1       0.87      0.87      0.87        23\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.86      0.86      0.86        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n",
      "Test Data Confusion Matrix:\n",
      " [[16  3]\n",
      " [ 3 20]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "\n",
    "scaler = None\n",
    "# fit the classification models \n",
    "for m in models:\n",
    "    print('fastText en-pretrained', m)\n",
    "\n",
    "    if isinstance(m, MultinomialNB):\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        scaler = None\n",
    "\n",
    "    classify(ft_vectorizer, m, X_train.new_text, y_train.asd, X_test.new_text, y_test.asd, scaler=scaler)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word2vec embedings trained on the current dataset, performed better than the out of the box, pretrained model available in Gensim. Accuracy for the models:\n",
    "\n",
    "0.81 - for KNN ( training data was 0.90 => overfit)<br>\n",
    "0.79 - multinomial naive bayes (training data 0.87 => overfit)<br>\n",
    "0.81 - for Random Forest ( training data 0.91 => overfit)<br>\n",
    "0.83 - SVM linear kernel (training data 0.89 => overfit)<br>\n",
    "0.86 - SVM RBF kernel ( training data 0.91 =>maybe  overfit)<br>\n",
    "\n",
    "FastText Crawl pretrained english vectors resulted in low performance, with most model overfitting the training data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Approach 5b. FastText model build on the current Reddit dataset</b>\n",
    "\n",
    "\n",
    "https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "\n",
    "FastText uses a simple neural network architecture for text classification. It consists of an input layer (for the text representation), a hidden layer, and an output layer. The output layer has as many neurons as there are categories, and it uses softmax activation to compute class probabilities (default).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: \n",
      "      asd   asd_label\n",
      "116    1  __label__1\n",
      "293    0  __label__0\n",
      "\n",
      "Test labels: \n",
      "      asd   asd_label\n",
      "231    0  __label__0\n",
      "243    1  __label__1\n"
     ]
    }
   ],
   "source": [
    "# fastText requries text data to be formated this way: \n",
    "# __label__category text\n",
    "y_train['asd_label'] = '__label__' + y_train.asd.astype(str)\n",
    "y_test['asd_label'] = '__label__' + y_test.asd.astype(str)\n",
    "print('Train labels: \\n', y_train.head(2))\n",
    "print('\\nTest labels: \\n', y_test.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: \n",
      " 116    __label__1 womenonthespectrum womenonthespectr...\n",
      "293    __label__0 Owl House Owl House subreddit Disne...\n",
      "452    __label__1 autism speak Sucks autism speak suc...\n",
      "dtype: object\n",
      "\n",
      "Test labels: \n",
      " 231    __label__0 Gamingcirclejerk gaming Circlejerk ...\n",
      "243    __label__1 ASDpeersupport ASD Peer support Pee...\n",
      "392    __label__0 selfharm Subreddit Self Harmers sub...\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# save data for fastText training in a single dataframe\n",
    "df_asd_newtext_train = y_train.asd_label + ' ' + X_train.new_text \n",
    "# save data for fastText test in a single dataframe\n",
    "df_asd_newtext_test = y_test.asd_label + ' ' + X_test.new_text \n",
    "\n",
    "print('Train labels: \\n', df_asd_newtext_train.head(3))\n",
    "print('\\nTest labels: \\n', df_asd_newtext_test.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write data to text file - fastText required format\n",
    "# this will be used later\n",
    "df_asd_newtext_train.to_csv('annotated_data/data_fastText/asd_newtext_train',index=False, header=False )\n",
    "df_asd_newtext_test.to_csv('annotated_data/data_fastText/asd_newtext_test',index=False, header=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'annotated_data/data_fastText/asd_newtext_train'\n",
    "test_data_path = 'annotated_data/data_fastText/asd_newtext_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>asd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>__label__1 womenonthespectrum womenonthespectr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>__label__0 Owl House Owl House subreddit Disne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>__label__1 autism speak Sucks autism speak suc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  asd\n",
       "116  __label__1 womenonthespectrum womenonthespectr...    1\n",
       "293  __label__0 Owl House Owl House subreddit Disne...    0\n",
       "452  __label__1 autism speak Sucks autism speak suc...    1"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get fastText ready labeled data - training\n",
    "ft_train_df = pd.concat([df_asd_newtext_train, y_train.asd], axis=1)\n",
    "ft_train_df = ft_train_df.rename(columns={0: 'text'})\n",
    "ft_train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>asd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>__label__0 Gamingcirclejerk gaming Circlejerk ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>__label__1 ASDpeersupport ASD Peer support Pee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>__label__0 selfharm Subreddit Self Harmers sub...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  asd\n",
       "231  __label__0 Gamingcirclejerk gaming Circlejerk ...    0\n",
       "243  __label__1 ASDpeersupport ASD Peer support Pee...    1\n",
       "392  __label__0 selfharm Subreddit Self Harmers sub...    0"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get fastText ready labeled data - testing\n",
    "ft_test_df = pd.concat([df_asd_newtext_test, y_test.asd], axis=1)\n",
    "ft_test_df = ft_test_df.rename(columns={0: 'text'})\n",
    "ft_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tempfile\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train/tune a fasttext model\n",
    "def train_ft_model(hyperparams):\n",
    "    # Initialize StratifiedKFold for cross-validation\n",
    "    k_folds = 5\n",
    "    kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=7)\n",
    "\n",
    "    # lists to store evaluation metrics for each fold\n",
    "    number_samples = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for train_index, val_index in kf.split(ft_train_df, ft_train_df.asd):\n",
    "        # split train data into train and validaton sets\n",
    "        train_data = ft_train_df.iloc[train_index]\n",
    "        val_data = ft_train_df.iloc[val_index]\n",
    "\n",
    "        # create temporary training and validation data files for this fold\n",
    "        with tempfile.NamedTemporaryFile(delete=False, mode='w') as train_file, \\\n",
    "            tempfile.NamedTemporaryFile(delete=False, mode='w') as val_file:\n",
    "\n",
    "            # Write the data to temporary files\n",
    "            train_data.text.to_csv(train_file, sep='\\t', header=False, index=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "            val_data.text.to_csv(val_file, sep='\\t', header=False, index=False, quoting=csv.QUOTE_NONE, escapechar=' ')\n",
    "\n",
    "            # file paths of temporary files ( the str name of the path )\n",
    "            train_file_path = train_file.name\n",
    "            val_file_path = val_file.name\n",
    "\n",
    "        # Create and train a FastText model\n",
    "        model = fasttext.train_supervised(\n",
    "            input=train_file_path,  # Specify your training data file\n",
    "            **hyperparams  # Pass the parameters\n",
    "        )\n",
    "\n",
    "        # evaluate the model on the validation data\n",
    "        # The output are the number of samples, the precision at one and the recall at one (https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "        result = model.test(val_file_path) \n",
    "        print(result)\n",
    "\n",
    "        # Collect evaluation metrics\n",
    "        number_samples.append(result[0])\n",
    "        precision_scores.append(result[1])\n",
    "        recall_scores.append(result[2])\n",
    "\n",
    "        # remove temporary files\n",
    "        os.remove(train_file_path)\n",
    "        os.remove(val_file_path)\n",
    "\n",
    "    # compute and print average evaluation metrics\n",
    "\n",
    "    avg_precision = np.mean(precision_scores)\n",
    "    avg_recall = np.mean(recall_scores)\n",
    "\n",
    "\n",
    "    print(f\"Samples in each fold: {[i for i in number_samples]}\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 0.1 | epoch = 5\n",
      "(34, 0.7941176470588235, 0.7941176470588235)\n",
      "(34, 0.7941176470588235, 0.7941176470588235)\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(33, 0.8181818181818182, 0.8181818181818182)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8217\n",
      "Average Recall: 0.8217\n",
      "lr = 0.1 | epoch = 10\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.7941176470588235, 0.7941176470588235)\n",
      "(34, 0.8529411764705882, 0.8529411764705882)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8396\n",
      "Average Recall: 0.8396\n",
      "lr = 0.1 | epoch = 15\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.8823529411764706, 0.8823529411764706)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8453\n",
      "Average Recall: 0.8453\n",
      "lr = 0.1 | epoch = 20\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8631\n",
      "Average Recall: 0.8631\n",
      "lr = 0.5 | epoch = 5\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.8235294117647058, 0.8235294117647058)\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8631\n",
      "Average Recall: 0.8631\n",
      "lr = 0.5 | epoch = 10\n",
      "(34, 0.8529411764705882, 0.8529411764705882)\n",
      "(34, 0.8823529411764706, 0.8823529411764706)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8806\n",
      "Average Recall: 0.8806\n",
      "lr = 0.5 | epoch = 15\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "(33, 0.8181818181818182, 0.8181818181818182)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8982\n",
      "Average Recall: 0.8982\n",
      "lr = 0.5 | epoch = 20\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.9102\n",
      "Average Recall: 0.9102\n",
      "lr = 1.0 | epoch = 5\n",
      "(34, 0.8529411764705882, 0.8529411764705882)\n",
      "(34, 0.8823529411764706, 0.8823529411764706)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.8806\n",
      "Average Recall: 0.8806\n",
      "lr = 1.0 | epoch = 10\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.9102\n",
      "Average Recall: 0.9102\n",
      "lr = 1.0 | epoch = 15\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.9090909090909091, 0.9090909090909091)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.9223\n",
      "Average Recall: 0.9223\n",
      "lr = 1.0 | epoch = 20\n",
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.9090909090909091, 0.9090909090909091)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.9223\n",
      "Average Recall: 0.9223\n"
     ]
    }
   ],
   "source": [
    "lrs = [0.1, 0.5, 1.0]\n",
    "epochs = [5, 10, 15, 20]\n",
    "\n",
    "for lr in lrs:\n",
    "    for epoch in epochs:\n",
    "        print(f'lr = {lr} | epoch = {epoch}')\n",
    "        # set FastText parameters\n",
    "        hyperparams = {\n",
    "            \"lr\": lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"wordNgrams\": 2,\n",
    "            'dim':300,\n",
    "            'ws':3,\n",
    "            'thread':1\n",
    "        }\n",
    "        ffasttext_model = train_ft_model(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34, 0.9411764705882353, 0.9411764705882353)\n",
      "(34, 0.9117647058823529, 0.9117647058823529)\n",
      "(34, 0.9705882352941176, 0.9705882352941176)\n",
      "(33, 0.8787878787878788, 0.8787878787878788)\n",
      "(33, 0.8484848484848485, 0.8484848484848485)\n",
      "Samples in each fold: [34, 34, 34, 33, 33]\n",
      "Average Precision: 0.9102\n",
      "Average Recall: 0.9102\n"
     ]
    }
   ],
   "source": [
    "# set FastText parameters - best performance on test set data\n",
    "lr = 1.0 \n",
    "epoch = 10\n",
    "\n",
    "hyperparams = {\n",
    "    \"lr\": lr,\n",
    "    \"epoch\": epoch,\n",
    "    \"wordNgrams\": 2,\n",
    "    'dim':300,\n",
    "    'ws':3,\n",
    "    'thread':1\n",
    "}\n",
    "ft_model = train_ft_model(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5596"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocabulary size\n",
    "len(ft_model.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\\t\" + str(N))\n",
    "    print(\"Precision @{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"Recall @{}\\t{:.3f}\".format(1, r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data performance\n",
      "N\t\t42\n",
      "Precision @1\t0.905\n",
      "Recall @1\t0.905\n"
     ]
    }
   ],
   "source": [
    "# the output are the number of samples (here 168), the precision at one (0.87) and the recall at one (0.87)\n",
    "# https://fasttext.cc/docs/en/supervised-tutorial.html\n",
    "# train data\n",
    "print('Test data performance')\n",
    "print_results(*ft_model.test('annotated_data/data_fastText/asd_newtext_test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average performance for the FastText model - trained on current data:\n",
    "test precission = 0.90 (training set is 0.91 - not overfit)\n",
    "test recall = 0.90 ( training set is 0.91 - not overfit )\n",
    "\n",
    "FastText performed the best out of the word emebeding models. The advantage of fastText is also that it deals with the OOV problem fairly well, this this model will be prefered to the count frequency ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>asd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>__label__0 Gamingcirclejerk gaming Circlejerk ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>__label__1 ASDpeersupport ASD Peer support Pee...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  asd\n",
       "231  __label__0 Gamingcirclejerk gaming Circlejerk ...    0\n",
       "243  __label__1 ASDpeersupport ASD Peer support Pee...    1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for text in ft_test_df['text']:\n",
    "    prediction = ft_model.predict(text)[0][0]  \n",
    "    predictions.append(prediction)\n",
    "last_characters = [int(prediction[-1]) for prediction in predictions]\n",
    "# last_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__0 cod AW Subreddit move Callof Duty subreddit move Callof Duty Subreddit move Callof Duty',\n",
       " '__label__0 ndwitches ndwitche subreddit neurodivergent practitioner witchcraft like witchesvspatriarchy ND people',\n",
       " '__label__0 downsyndrome downsyndrome community thing relate syndrome let s place positive hateful disrespectful submission comment remove community thing relate syndrome let s place positive hateful disrespectful submission comment remove subreddit find interesting autism autistic asperger disability 22q useful link National Syndrome Society http www ndss org',\n",
       " '__label__0 Neurodivergent Languag Neurodivergent Languag reddit dedicate discussion creation language easy people communication emotional issue use']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels = ft_test_df.asd\n",
    "\n",
    "misclassified_texts = []\n",
    "\n",
    "# identify misclassified rows\n",
    "for predicted_label, true_label, text in zip(last_characters, true_labels, ft_test_df.text):\n",
    "    if predicted_label != true_label:\n",
    "        misclassified_texts.append(text) \n",
    "        misclassified_true_labels.append(true_label)\n",
    "        \n",
    "misclassified_texts \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9998947978019714, 'autistic'),\n",
       " (0.9998741149902344, 'Autism'),\n",
       " (0.9997900128364563, '</s>'),\n",
       " (0.9997149705886841, 'people'),\n",
       " (0.9996727108955383, 'Autistic'),\n",
       " (0.9996564984321594, 'spectrum'),\n",
       " (0.9996528625488281, 'Neurodivergent'),\n",
       " (0.9993544220924377, 'ASD'),\n",
       " (0.9993276596069336, 'community'),\n",
       " (0.999090850353241, 'Aspergers')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the model trained on current data set results in good similar words to 'autism'\n",
    "ft_model.get_nearest_neighbors('autism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "ft_model.save_model('models/fastText_subreddits.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Deep learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 4. 5. 6. Preprocess, Tokenize, Vectorize, Apply Classifier and Evaluate the model - uses transformers and pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be split into training and testing data sets that will be saved and imported in google Colab - this is done for consistency of the datasets (in google colab, when using a GPU, the random seed will not generate the same result as this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asd</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1</td>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1</td>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0</td>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "      <td>autisticpeople Autism Spectrum Disorder News l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>1</td>\n",
       "      <td>Autistic Pride2 Autistic Pride2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     asd                                               text\n",
       "205    1  autismmemes autism memes Meme for people with ...\n",
       "488    1  ASDcareers The Careers of People with ASD Livi...\n",
       "231    0  Gamingcirclejerk Gaming Circlejerk Don Cheadle...\n",
       "104    1  autisticpeople Autism Spectrum Disorder News l...\n",
       "263    1                    Autistic Pride2 Autistic Pride2"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_annotated_df_short.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Other</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Other  ASD\n",
       "205      0    1\n",
       "488      0    1\n",
       "231      1    0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(asd_subs_annotated_df_short['asd'])\n",
    "# Rename columns\n",
    "col_names = {0: 'Other', 1: 'ASD'}\n",
    "df_encoded.rename(columns=col_names, inplace=True)\n",
    "df_encoded = df_encoded.astype(int)\n",
    "df_encoded.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Other</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>autismmemes autism memes Meme for people with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>ASDcareers The Careers of People with ASD Livi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  Other  ASD\n",
       "205  autismmemes autism memes Meme for people with ...      0    1\n",
       "488  ASDcareers The Careers of People with ASD Livi...      0    1\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...      1    0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([asd_subs_annotated_df_short, df_encoded], axis=1)\n",
    "df.drop(['asd'], axis=1, inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train set ( 0.8 of the dataset ) and test set of (0.2 of the dataset)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, stratify=df['ASD'], random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Other</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>womenonthespectrum womenonthespectrum A commun...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>The Owl House The Owl House A subreddit for th...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Autism Speaks Sucks Autism Speaks Sucks A comm...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  Other  ASD\n",
       "116  womenonthespectrum womenonthespectrum A commun...      0    1\n",
       "293  The Owl House The Owl House A subreddit for th...      1    0\n",
       "452  Autism Speaks Sucks Autism Speaks Sucks A comm...      0    1"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Other</th>\n",
       "      <th>ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Gamingcirclejerk Gaming Circlejerk Don Cheadle...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>ASDpeersupport ASD Peer Support Peer based sup...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>selfharm A Subreddit for Self Harmers A subred...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  Other  ASD\n",
       "231  Gamingcirclejerk Gaming Circlejerk Don Cheadle...      1    0\n",
       "243  ASDpeersupport ASD Peer Support Peer based sup...      0    1\n",
       "392  selfharm A Subreddit for Self Harmers A subred...      1    0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('annotated_data/asd_subs_roberta/train_data.csv')\n",
    "test_data.to_csv('annotated_data/asd_subs_roberta/test_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text classificatoin with RoBERTa - pytorch and transformers\n",
    "\n",
    "The RoBERTa model requires more computeational power and I have used google free GPU available via Colab. \n",
    "\n",
    "Due to the size of RoBERTa model and the GPU limitation for the free tier of Google Clolab, the distilled version of this model (distilroberta-base) was used for training: https://huggingface.co/distilroberta-base\n",
    "\n",
    "Notebook avaialble at:\n",
    "https://colab.research.google.com/drive/1kW2o_FW3R7cTpHidwvZDpAD_QtEfG4jQ#scrollTo=o8gtVVPhBKzk&uniqifier=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distilroberta model resulted in similar result as the fasttext model. However, the distilroberta model is larger and more computationally expensive ( for faster performace it requiers a GPU ). Due to this reason , the lighter , easier to use fasttext model will be used in the final classificaiton ( when the models show similar results, a wise choice should be the simpler, easier to implement model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subreddits classification with fastText\n",
    "includes the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Representation Autism Awareness A place...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>Aspie Positive Aspie Positive by aspies for as...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism Trump Autism Trump Autism Trump A...</td>\n",
       "      <td>lv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "\n",
       "                                                text lang  \n",
       "0  Autism Representation Autism Awareness A place...   en  \n",
       "1  Aspie Positive Aspie Positive by aspies for as...   en  \n",
       "2  Trump Autism Trump Autism Trump Autism Trump A...   lv  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__1',), array([0.99910378])) AutismRepresentation\n",
      "(('__label__1',), array([0.55182385])) AspiePositive\n",
      "(('__label__1',), array([1.00000727])) TrumpAutism\n",
      "(('__label__0',), array([0.78413045])) nevergrewup\n",
      "(('__label__1',), array([0.97874802])) asperger\n",
      "(('__label__1',), array([0.7675482])) AutismTranslator\n",
      "(('__label__0',), array([0.70082128])) gutscience\n",
      "(('__label__1',), array([0.55717224])) artisticspectrum\n",
      "(('__label__1',), array([0.88951808])) AutisticVents\n",
      "(('__label__1',), array([0.9952963])) Autism___Parenting\n",
      "(('__label__0',), array([0.6394068])) martincabello\n",
      "(('__label__1',), array([0.69534355])) AutisticWorkersUnion\n",
      "(('__label__0',), array([0.97005862])) NonBinary\n",
      "(('__label__1',), array([0.8873623])) NTHusbandASWife\n",
      "(('__label__1',), array([0.79995632])) AspergersParents\n",
      "(('__label__1',), array([0.85324061])) asdparents\n",
      "(('__label__1',), array([0.99582934])) AspergersAfterDark\n",
      "(('__label__1',), array([0.75822228])) AskAnAutistic\n",
      "(('__label__0',), array([0.65962857])) PoliticalCompassMemes\n",
      "(('__label__1',), array([0.87769139])) cpunk\n",
      "(('__label__0',), array([0.66796821])) MechanicAdvice\n",
      "(('__label__1',), array([0.82436138])) AutasticJobs\n",
      "(('__label__1',), array([0.88613325])) NeurodivergentWomen\n",
      "(('__label__0',), array([0.98612314])) cogsci\n",
      "(('__label__0',), array([0.87808973])) aspergers\n",
      "(('__label__1',), array([0.90226787])) theNDfamily\n",
      "(('__label__0',), array([0.61625487])) TheNoiseFloorAV\n",
      "(('__label__0',), array([0.75466609])) schizophrenia\n",
      "(('__label__0',), array([0.72122228])) SampleSize\n",
      "(('__label__0',), array([0.65535259])) Explainlikeimscared\n",
      "(('__label__1',), array([0.91851646])) AUTZtoken\n",
      "(('__label__1',), array([0.95031768])) chessbergers\n",
      "(('__label__0',), array([0.67439538])) NMOdisease\n",
      "(('__label__0',), array([0.93045342])) antiwork\n",
      "(('__label__1',), array([0.71611029])) AreTheNTsOK\n",
      "(('__label__1',), array([0.86806774])) XenogendersAndMore\n",
      "(('__label__1',), array([0.83106309])) Asdhd\n",
      "(('__label__0',), array([0.7481342])) TrueUnpopularOpinion\n",
      "(('__label__1',), array([0.98063487])) another_autism_sub\n",
      "(('__label__1',), array([0.97998768])) AutisticswPDs\n",
      "(('__label__1',), array([0.99320859])) AutisticCoded\n",
      "(('__label__1',), array([0.99563414])) AutismIsCool\n",
      "(('__label__0',), array([0.6627745])) SuicidePreventionRes\n",
      "(('__label__0',), array([0.77641976])) SuicideWatch\n",
      "(('__label__1',), array([0.9355576])) AutismCrusade\n",
      "(('__label__0',), array([0.87777638])) Bad_Cop_No_Donut\n",
      "(('__label__1',), array([0.84790331])) aretheNTsokay\n",
      "(('__label__0',), array([0.87812519])) todayilearned\n",
      "(('__label__1',), array([0.97460896])) AutismTraumaSurvivors\n",
      "(('__label__1',), array([0.99979478])) AutismVentRant\n",
      "(('__label__1',), array([0.99478358])) aspieselfies\n",
      "(('__label__0',), array([0.93762827])) intj\n",
      "(('__label__1',), array([0.8164013])) ASDTrans\n",
      "(('__label__1',), array([0.95244753])) AutisticGuild\n",
      "(('__label__1',), array([0.92983073])) autismspectrum_memes\n",
      "(('__label__0',), array([0.52306199])) offmychest\n",
      "(('__label__0',), array([0.65492511])) aspergirls\n",
      "(('__label__0',), array([0.75711894])) CuratedTumblr\n",
      "(('__label__0',), array([0.73410851])) Health\n",
      "(('__label__0',), array([0.99794161])) mentalpod\n",
      "(('__label__0',), array([0.98245239])) exmormon\n",
      "(('__label__0',), array([0.84725064])) GBL\n",
      "(('__label__0',), array([0.70052093])) interstem\n",
      "(('__label__0',), array([0.74627405])) teenagers\n",
      "(('__label__1',), array([0.5214113])) RadicallyOpenDBT\n",
      "(('__label__1',), array([0.94303828])) NeurodivergentLGBTQ\n",
      "(('__label__1',), array([0.86698318])) neurodivergentyouth\n",
      "(('__label__0',), array([0.83116114])) SpaceXLounge\n",
      "(('__label__0',), array([0.62731779])) AvoidSuicide\n",
      "(('__label__0',), array([0.82346016])) Autistic\n",
      "(('__label__0',), array([0.62175876])) DontKillYourself\n",
      "(('__label__0',), array([0.95402259])) breakingmom\n",
      "(('__label__1',), array([0.7166757])) AutisticLadies\n",
      "(('__label__1',), array([0.99903834])) AutisticFurs\n",
      "(('__label__1',), array([0.55198449])) AutismAfterDark\n",
      "(('__label__1',), array([0.7007612])) autismcirclejerk\n",
      "(('__label__0',), array([0.9671582])) dogswithjobs\n",
      "(('__label__1',), array([0.55359381])) AnatolianShepherdDogs\n",
      "(('__label__0',), array([0.65409201])) lgbt\n",
      "(('__label__1',), array([0.94064623])) mildautism\n",
      "(('__label__1',), array([0.89115369])) AutismTranslated\n",
      "(('__label__0',), array([0.75174212])) SJSN\n",
      "(('__label__1',), array([0.75538331])) Neuropals\n",
      "(('__label__1',), array([1.00001001])) AutismScotland\n",
      "(('__label__1',), array([0.90670305])) AutiesWhoSurvived\n",
      "(('__label__1',), array([0.93958539])) autisticwitches\n",
      "(('__label__1',), array([0.97413236])) AutisticConversation\n",
      "(('__label__1',), array([1.00000656])) OnionsCauseAutism\n",
      "(('__label__1',), array([0.90946102])) WritteND\n",
      "(('__label__1',), array([0.99637169])) AutisticRainbow\n",
      "(('__label__1',), array([0.99974805])) VidarAutism\n",
      "(('__label__0',), array([0.50175446])) Shaqccinescauseautism\n",
      "(('__label__1',), array([0.73232967])) Autistic_ADHDwCPTSD\n",
      "(('__label__0',), array([0.87429011])) RoastMe\n",
      "(('__label__0',), array([0.91961569])) IAmA\n",
      "(('__label__0',), array([0.76821154])) AMA\n",
      "(('__label__1',), array([0.86188775])) AutisticReddit\n",
      "(('__label__1',), array([0.97097486])) Transpies\n",
      "(('__label__0',), array([0.8903746])) CPTSD\n",
      "(('__label__1',), array([0.99999702])) AutisticNationalism\n",
      "(('__label__1',), array([0.51028842])) HighFunctioningBP\n",
      "(('__label__1',), array([0.95965081])) autisticpeople\n",
      "(('__label__1',), array([0.63158309])) ADHD_NoHoldsBarred\n",
      "(('__label__0',), array([0.82028615])) chd\n",
      "(('__label__0',), array([0.78952569])) anchorage\n",
      "(('__label__0',), array([0.52942544])) AspieShowcase\n",
      "(('__label__0',), array([0.73691291])) CoronavirusUK\n",
      "(('__label__1',), array([0.98687857])) AutisticPride\n",
      "(('__label__1',), array([0.93469977])) Aspie\n",
      "(('__label__1',), array([0.91075104])) thyroidandaspergers\n",
      "(('__label__1',), array([0.92298394])) AutisticMercury\n",
      "(('__label__0',), array([0.91459358])) askscience\n",
      "(('__label__1',), array([0.99784595])) TheAspieWorld\n",
      "(('__label__1',), array([0.98054552])) womenonthespectrum\n",
      "(('__label__1',), array([0.9507556])) AutisticWithADHD\n",
      "(('__label__1',), array([0.95354462])) AutisticFTM\n",
      "(('__label__1',), array([0.86786848])) Autism_Pride\n",
      "(('__label__1',), array([0.99124342])) aspergersbooks\n",
      "(('__label__0',), array([0.93945533])) Nootropics\n",
      "(('__label__0',), array([0.71094811])) NoStupidQuestions\n",
      "(('__label__1',), array([0.92787856])) AutismAdult\n",
      "(('__label__0',), array([0.54183263])) AspieR4R\n",
      "(('__label__1',), array([0.89666688])) SkypeCallAutism\n",
      "(('__label__1',), array([0.95332354])) ASDADHD\n",
      "(('__label__1',), array([0.92490584])) ASD_Gaming\n",
      "(('__label__0',), array([0.88524264])) vaxxhappened\n",
      "(('__label__1',), array([0.65504795])) INTPAspBridge\n",
      "(('__label__0',), array([0.8831982])) Futurology\n",
      "(('__label__1',), array([0.98043108])) ArcticAutism\n",
      "(('__label__0',), array([0.76551956])) AskMeAnythingIAnswer\n",
      "(('__label__1',), array([0.88917589])) antineurodiversity\n",
      "(('__label__1',), array([0.82967317])) AspiesZoom\n",
      "(('__label__1',), array([0.66140223])) NDLifeProTips\n",
      "(('__label__0',), array([0.57937419])) PrequelMemes\n",
      "(('__label__1',), array([0.95063752])) AutismAustralia\n",
      "(('__label__1',), array([0.9953872])) AnythingButAutistic\n",
      "(('__label__0',), array([0.74895251])) 22q\n",
      "(('__label__0',), array([0.56874096])) mentalhealthmommies\n",
      "(('__label__0',), array([0.62224686])) MentalGold\n",
      "(('__label__1',), array([0.74078155])) Autistic_Metalheads\n",
      "(('__label__1',), array([0.99270755])) ASD_irl\n",
      "(('__label__0',), array([0.78573954])) RomanceBooks\n",
      "(('__label__1',), array([0.61231512])) AutismInWomen\n",
      "(('__label__1',), array([0.91570437])) AutismSiblings\n",
      "(('__label__1',), array([0.92935842])) Autistic_safe_space\n",
      "(('__label__1',), array([0.89255548])) AutismInTheWorkplace\n",
      "(('__label__0',), array([0.90082568])) socialskills\n",
      "(('__label__0',), array([0.75935245])) AmItheAsshole\n",
      "(('__label__1',), array([0.65047807])) NTsBeingBros\n",
      "(('__label__1',), array([0.70409667])) HighFunctioningStoner\n",
      "(('__label__1',), array([0.99998748])) ScienceOnAutism\n",
      "(('__label__1',), array([0.98073173])) AutisticGalsnPals\n",
      "(('__label__1',), array([0.99506992])) FuckAutismSpeaks\n",
      "(('__label__1',), array([0.99732488])) AutismPride\n",
      "(('__label__0',), array([0.85843492])) FanTheories\n",
      "(('__label__1',), array([0.89940494])) FriendsOnTheSpectrum\n",
      "(('__label__1',), array([0.50046313])) ndmarxism\n",
      "(('__label__1',), array([0.97459513])) AutismResearch\n",
      "(('__label__1',), array([0.993011])) autistic_irl\n",
      "(('__label__1',), array([0.9997676])) AutisticDarkWeb\n",
      "(('__label__1',), array([0.98534405])) NeurodivergentFitness\n",
      "(('__label__0',), array([0.83585775])) trees\n",
      "(('__label__1',), array([0.8182469])) RedditHasAutism\n",
      "(('__label__1',), array([0.91477728])) NeurodivergentMemes\n",
      "(('__label__0',), array([0.68971449])) memes\n",
      "(('__label__0',), array([0.75763643])) unpopularopinion\n",
      "(('__label__1',), array([0.98660117])) MyAutisticChild\n",
      "(('__label__1',), array([0.9988538])) BPDandAutism\n",
      "(('__label__0',), array([0.82570404])) SubredditDrama\n",
      "(('__label__0',), array([0.56302184])) HomeDepot\n",
      "(('__label__0',), array([0.93265986])) science\n",
      "(('__label__0',), array([0.64527601])) Prosopagnosia\n",
      "(('__label__0',), array([0.90549445])) lawschooladmissions\n",
      "(('__label__0',), array([0.53137928])) UnboxedNetwork\n",
      "(('__label__0',), array([0.88150287])) suggestmeabook\n",
      "(('__label__1',), array([0.67001092])) AutismCentral\n",
      "(('__label__0',), array([0.78980643])) INTP\n",
      "(('__label__0',), array([0.88611662])) slatestarcodex\n",
      "(('__label__1',), array([0.92010015])) AutisticMusicians\n",
      "(('__label__0',), array([0.96443146])) AskHistorians\n",
      "(('__label__0',), array([0.76586515])) MadeMeSmile\n",
      "(('__label__1',), array([0.50594628])) TypedHQ\n",
      "(('__label__0',), array([0.60916281])) BarryBeeBenson\n",
      "(('__label__1',), array([0.75626719])) ChristianAspies\n",
      "(('__label__0',), array([0.97293502])) reddit.com\n",
      "(('__label__1',), array([0.99852067])) SureFriendAutism\n",
      "(('__label__1',), array([0.8612231])) AutisticAdults\n",
      "(('__label__0',), array([0.8092193])) BPD\n",
      "(('__label__1',), array([0.98405838])) AutismImprovement\n",
      "(('__label__1',), array([0.77738786])) AttorneyWoo\n",
      "(('__label__1',), array([0.99865597])) autism_controversial\n",
      "(('__label__1',), array([0.93910474])) ReallyAutistic\n",
      "(('__label__1',), array([0.87247539])) pddnos\n",
      "(('__label__0',), array([0.83940339])) Scholar\n",
      "(('__label__1',), array([0.9318738])) ASD_republic\n",
      "(('__label__0',), array([0.99538404])) atheism\n",
      "(('__label__1',), array([0.7687788])) fasd\n",
      "(('__label__1',), array([0.75329298])) aspiechristian\n",
      "(('__label__1',), array([0.97726643])) QueerAspies\n",
      "(('__label__1',), array([0.88050872])) ASDrelationships\n",
      "(('__label__0',), array([0.76739144])) AskWomen\n",
      "(('__label__1',), array([0.74929732])) asptrees\n",
      "(('__label__1',), array([0.99986291])) autismmemes\n",
      "(('__label__0',), array([0.75268108])) LandmarkCollege\n",
      "(('__label__0',), array([0.99461466])) wallstreetbets\n",
      "(('__label__1',), array([0.50756633])) autism\n",
      "(('__label__1',), array([0.88664472])) NeurodivergentSaints\n",
      "(('__label__1',), array([0.99861568])) VaxxCauseAutism\n",
      "(('__label__1',), array([0.98824173])) ASD_Programmers\n",
      "(('__label__1',), array([0.89623028])) ASDirect\n",
      "(('__label__0',), array([0.58463156])) TeensWithAspergers\n",
      "(('__label__1',), array([0.99980599])) aspergerinterests\n",
      "(('__label__1',), array([0.99662346])) LGBTautism\n",
      "(('__label__1',), array([0.92525309])) gaspies\n",
      "(('__label__1',), array([0.75944644])) SingleParentsASDKids\n",
      "(('__label__1',), array([0.99932361])) AutisticComedy\n",
      "(('__label__1',), array([0.98724723])) CouncilOfAutism\n",
      "(('__label__1',), array([0.9995153])) autism_help\n",
      "(('__label__1',), array([0.65566063])) Au2gether\n",
      "(('__label__0',), array([0.60895699])) portymemesv2\n",
      "(('__label__0',), array([0.8678174])) pyroluria\n",
      "(('__label__0',), array([0.81178474])) relationships\n",
      "(('__label__1',), array([0.7897861])) AutisticDatingTips\n",
      "(('__label__1',), array([0.95543498])) AutisticMastery\n",
      "(('__label__1',), array([0.66981798])) AspieOppositeSex\n",
      "(('__label__0',), array([0.9144513])) spacex\n",
      "(('__label__0',), array([0.92578602])) ftm\n",
      "(('__label__0',), array([0.84328461])) Gamingcirclejerk\n",
      "(('__label__1',), array([0.99987102])) MadeOfAutism\n",
      "(('__label__0',), array([0.97116137])) CBD\n",
      "(('__label__1',), array([0.787184])) AspieGirls\n",
      "(('__label__1',), array([0.93486154])) evilautism\n",
      "(('__label__0',), array([0.69096404])) hypersensitivity\n",
      "(('__label__1',), array([0.9422518])) iamveryautistic\n",
      "(('__label__0',), array([0.80765867])) bipolar2\n",
      "(('__label__1',), array([0.90138495])) Autism_Parenting\n",
      "(('__label__0',), array([0.92763352])) TwoXChromosomes\n",
      "(('__label__1',), array([0.63344669])) AutisticAndTrans\n",
      "(('__label__1',), array([0.79601836])) rabiespride\n",
      "(('__label__1',), array([0.58686298])) ASDpeersupport\n",
      "(('__label__1',), array([0.80676919])) aspieconversations\n",
      "(('__label__1',), array([0.8720156])) TodaysHyperfocus\n",
      "(('__label__1',), array([0.71934044])) AspergersFriends\n",
      "(('__label__1',), array([0.74772519])) weedcuresautism\n",
      "(('__label__1',), array([0.89178246])) MNeurodivergent\n",
      "(('__label__1',), array([0.88788396])) asdmovieclub\n",
      "(('__label__1',), array([0.93611366])) AutisticTanks\n",
      "(('__label__1',), array([0.98986357])) Drama\n",
      "(('__label__1',), array([0.98775136])) UpvotedBecauseAutism\n",
      "(('__label__0',), array([0.87487346])) dating_advice\n",
      "(('__label__1',), array([0.86676478])) TheAutisticWomen\n",
      "(('__label__0',), array([0.60926247])) AspieAnimemes\n",
      "(('__label__0',), array([0.56996912])) lighttherapy\n",
      "(('__label__0',), array([0.70863974])) HF_BPD\n",
      "(('__label__0',), array([0.60492587])) AmITheAngel\n",
      "(('__label__1',), array([0.97009647])) autism_cr3w\n",
      "(('__label__1',), array([0.99889046])) AutismGals\n",
      "(('__label__1',), array([0.91741931])) AutismCPTSD\n",
      "(('__label__1',), array([0.59143978])) AutisticBets\n",
      "(('__label__1',), array([0.9999845])) AutisticPride2\n",
      "(('__label__1',), array([0.94039994])) Autism_Parents\n",
      "(('__label__1',), array([0.88641554])) NeurodivergentLesbian\n",
      "(('__label__1',), array([0.89861232])) SpicyAutism\n",
      "(('__label__0',), array([0.8920365])) AskEurope\n",
      "(('__label__1',), array([0.6977762])) neuroatypical\n",
      "(('__label__1',), array([0.67138672])) NeurodivergentQueers\n",
      "(('__label__0',), array([0.74559462])) AskMen\n",
      "(('__label__1',), array([0.95502597])) Neurodivergent\n",
      "(('__label__0',), array([0.94551319])) PurplePillDebate\n",
      "(('__label__1',), array([0.99912459])) AutismAndAddiction\n",
      "(('__label__1',), array([0.95780182])) rightwingauties\n",
      "(('__label__0',), array([0.85270709])) asexuality\n",
      "(('__label__1',), array([0.89939272])) AutismADHD\n",
      "(('__label__1',), array([0.99999094])) autisticlifehacks\n",
      "(('__label__0',), array([0.54144913])) DebateVaccines\n",
      "(('__label__1',), array([0.99998903])) autisticLosers\n",
      "(('__label__0',), array([0.65030223])) selectivemutism\n",
      "(('__label__1',), array([0.72584212])) HowAutismWorks\n",
      "(('__label__0',), array([0.74981081])) ForeverAlone\n",
      "(('__label__1',), array([1.00000155])) girl_parents_autism\n",
      "(('__label__0',), array([0.82605904])) SpecialNeeds\n",
      "(('__label__0',), array([0.61898702])) JJScalpers\n",
      "(('__label__0',), array([0.94927025])) 4chan\n",
      "(('__label__1',), array([0.95938557])) LateDiagnosedAutistic\n",
      "(('__label__1',), array([0.9988693])) Autism_Vent\n",
      "(('__label__0',), array([0.51212597])) SpaceXMasterrace\n",
      "(('__label__1',), array([0.98332095])) SpecialFixations\n",
      "(('__label__1',), array([1.00000215])) AutismArt\n",
      "(('__label__1',), array([0.99303859])) AutisticLiberation\n",
      "(('__label__0',), array([0.78121501])) TheOwlHouse\n",
      "(('__label__1',), array([0.99423742])) AutisticRants\n",
      "(('__label__0',), array([0.97389001])) askpsychology\n",
      "(('__label__1',), array([0.99390453])) ASDparenting\n",
      "(('__label__1',), array([0.69169259])) adhdmeme\n",
      "(('__label__1',), array([0.88482904])) autismgirls\n",
      "(('__label__1',), array([0.99601668])) ASDFriends\n",
      "(('__label__1',), array([1.00000799])) Autism_Gamers\n",
      "(('__label__0',), array([0.88830525])) specialed\n",
      "(('__label__1',), array([0.66977376])) EhlersDanlosMemes\n",
      "(('__label__1',), array([0.96469897])) AutisticPolitics\n",
      "(('__label__1',), array([0.76896805])) AuDHDWomen\n",
      "(('__label__0',), array([0.84667838])) NEET\n",
      "(('__label__1',), array([0.95274383])) AutisticPilking\n",
      "(('__label__0',), array([0.72980475])) writing\n",
      "(('__label__1',), array([0.99196333])) Parkourdude91\n",
      "(('__label__1',), array([0.75952339])) aspiememes\n",
      "(('__label__1',), array([0.95545191])) autism_central\n",
      "(('__label__0',), array([0.64477998])) adhdwomen\n",
      "(('__label__1',), array([0.99713928])) AutismPolitics\n",
      "(('__label__1',), array([0.94409066])) AutisticCompliance\n",
      "(('__label__1',), array([0.99904513])) radicalautism\n",
      "(('__label__1',), array([0.89886266])) autismlevel2and3\n",
      "(('__label__0',), array([0.60670829])) greentext\n",
      "(('__label__0',), array([0.83923078])) traaaaaaannnnnnnnnns\n",
      "(('__label__1',), array([0.99940979])) AutismParent\n",
      "(('__label__1',), array([0.9915328])) AutisticR4R\n",
      "(('__label__1',), array([0.70665616])) AspieZionism\n",
      "(('__label__1',), array([0.99979347])) AutisticCreatives\n",
      "(('__label__1',), array([0.70161939])) TruMentalcels\n",
      "(('__label__0',), array([0.9337666])) EverythingScience\n",
      "(('__label__1',), array([0.95912021])) AutisticLesbians\n",
      "(('__label__0',), array([0.76363742])) raisedbynarcissists\n",
      "(('__label__0',), array([0.56366074])) MemeMan1984\n",
      "(('__label__1',), array([0.99956852])) autism__friends\n",
      "(('__label__1',), array([0.84042299])) Aspergerswork\n",
      "(('__label__0',), array([0.97274333])) AcademicPsychology\n",
      "(('__label__1',), array([0.98423874])) Asperboys\n",
      "(('__label__0',), array([0.6884169])) psychologystudents\n",
      "(('__label__0',), array([0.80544418])) uniqueminds\n",
      "(('__label__1',), array([0.98009682])) aspiemusic\n",
      "(('__label__0',), array([0.72500783])) OccultConspiracy\n",
      "(('__label__1',), array([0.94721669])) AutisticPOC\n",
      "(('__label__1',), array([0.99321955])) VampireAutism\n",
      "(('__label__1',), array([0.95506763])) AspieMeetup\n",
      "(('__label__1',), array([0.99621797])) AutismPoetry\n",
      "(('__label__1',), array([0.77006316])) AspergersRiseUp\n",
      "(('__label__1',), array([0.76012433])) CodAW\n",
      "(('__label__1',), array([0.99426138])) LivingWithAutism\n",
      "(('__label__0',), array([0.91624606])) Mneumonese\n",
      "(('__label__1',), array([0.98790622])) AutisticMe\n",
      "(('__label__1',), array([0.68818581])) AutismBlogs\n",
      "(('__label__1',), array([1.00000966])) MaskedAutism\n",
      "(('__label__0',), array([0.91021961])) Showerthoughts\n",
      "(('__label__0',), array([0.58384281])) LightTherapies\n",
      "(('__label__1',), array([0.67324102])) ITCI\n",
      "(('__label__1',), array([0.67383993])) talesfromautism\n",
      "(('__label__1',), array([0.86605769])) Autism603\n",
      "(('__label__0',), array([0.76964182])) AskTeenGirls\n",
      "(('__label__0',), array([0.94040948])) HumanMicrobiome\n",
      "(('__label__0',), array([0.63904721])) bipolar\n",
      "(('__label__1',), array([0.97900665])) AutisticLBGTQ\n",
      "(('__label__1',), array([0.73492974])) AskAutism\n",
      "(('__label__1',), array([0.99901229])) TheAutismSpectrum\n",
      "(('__label__0',), array([0.69103539])) hyperlexia\n",
      "(('__label__1',), array([0.92052794])) DoYouHaveAutism\n",
      "(('__label__1',), array([0.94274056])) askpergers\n",
      "(('__label__0',), array([0.80969262])) executive_functioning\n",
      "(('__label__1',), array([0.80059111])) AutismVs2020\n",
      "(('__label__0',), array([0.6665253])) Specialism\n",
      "(('__label__0',), array([0.75643235])) antivax\n",
      "(('__label__1',), array([0.60081261])) SchizophreniaAlt\n",
      "(('__label__1',), array([0.96220219])) AutisticComrades\n",
      "(('__label__0',), array([0.88324505])) news\n",
      "(('__label__1',), array([0.63724709])) SpecialInterestEx\n",
      "(('__label__1',), array([0.99352854])) AutismFitness\n",
      "(('__label__1',), array([0.99786419])) livingwithaspergers\n",
      "(('__label__0',), array([0.96042889])) explainlikeimfive\n",
      "(('__label__1',), array([0.65441734])) EverythingAspergers\n",
      "(('__label__1',), array([0.8089897])) aspiekids\n",
      "(('__label__1',), array([0.74826556])) NeurodivergentPeace\n",
      "(('__label__1',), array([0.99918163])) LiteralAutism\n",
      "(('__label__1',), array([0.90726262])) AutismScreeches\n",
      "(('__label__0',), array([0.9114365])) dogecoin\n",
      "(('__label__0',), array([0.73089296])) TooAfraidToAsk\n",
      "(('__label__1',), array([0.9524439])) HelpWithAutism\n",
      "(('__label__0',), array([0.76121593])) depression\n",
      "(('__label__1',), array([0.60226649])) LGBTaspies\n",
      "(('__label__0',), array([0.7740593])) videos\n",
      "(('__label__1',), array([0.85171789])) NeurodivergentLondon\n",
      "(('__label__1',), array([0.99462903])) AgainstAutismSpeaks\n",
      "(('__label__1',), array([0.55519956])) PDAAutism\n",
      "(('__label__1',), array([0.99846196])) NatureIsAutistic\n",
      "(('__label__0',), array([0.95695484])) truscum\n",
      "(('__label__0',), array([0.59331185])) confession\n",
      "(('__label__1',), array([0.84163857])) Autistic_Women\n",
      "(('__label__1',), array([0.99865836])) AutisticADHD\n",
      "(('__label__1',), array([0.84978616])) AspieAsk\n",
      "(('__label__0',), array([0.79259557])) selfharm\n",
      "(('__label__1',), array([0.99973828])) ABarrelOfAutism\n",
      "(('__label__0',), array([0.88554102])) channelsofyoutube\n",
      "(('__label__0',), array([0.86677933])) genetics\n",
      "(('__label__1',), array([1.00001001])) WeaponizedAutism\n",
      "(('__label__0',), array([0.99211693])) 2021nflSuperBowl\n",
      "(('__label__0',), array([0.9178412])) ontario\n",
      "(('__label__1',), array([0.91949421])) AutisticPsychedelic\n",
      "(('__label__1',), array([0.9998709])) AutisticWorldbuilders\n",
      "(('__label__0',), array([0.87988627])) neuroscience\n",
      "(('__label__1',), array([0.9990744])) AutismIreland\n",
      "(('__label__0',), array([0.77600795])) ShitRedditSays\n",
      "(('__label__1',), array([1.00000787])) AutismUnited\n",
      "(('__label__0',), array([0.61497247])) TeenAspies\n",
      "(('__label__1',), array([0.98680443])) CripplingAutism\n",
      "(('__label__1',), array([0.97692478])) Autism_lgbt\n",
      "(('__label__1',), array([0.96171075])) Fidget_Spinner_Autism\n",
      "(('__label__1',), array([0.82697177])) AutisticIndians\n",
      "(('__label__1',), array([0.84886122])) NeurodivergentLanguag\n",
      "(('__label__1',), array([0.99820834])) NeurodivergentChat\n",
      "(('__label__1',), array([0.94624376])) bestof4chan\n",
      "(('__label__0',), array([0.87967277])) environment\n",
      "(('__label__0',), array([0.56126273])) aspergers_irl\n",
      "(('__label__1',), array([0.95901662])) NDWomen\n",
      "(('__label__1',), array([0.9545995])) AspiesUnite\n",
      "(('__label__1',), array([0.97511119])) TrueEvilAutism\n",
      "(('__label__0',), array([0.5013845])) TraumaAndPolitics\n",
      "(('__label__1',), array([0.77283686])) GamingWithAutism\n",
      "(('__label__0',), array([0.86265773])) WitchesVsPatriarchy\n",
      "(('__label__1',), array([0.8691535])) TheJesterOfAutism\n",
      "(('__label__1',), array([0.81333816])) TheSpectrumAsks\n",
      "(('__label__0',), array([0.8820864])) neurodiversity\n",
      "(('__label__1',), array([0.9696728])) athiests\n",
      "(('__label__0',), array([0.54543489])) ndrelationships\n",
      "(('__label__1',), array([0.75091988])) SpectrumwithAttitude\n",
      "(('__label__1',), array([0.95373499])) AspiePolitics\n",
      "(('__label__1',), array([0.81918788])) aspergers_dating\n",
      "(('__label__0',), array([0.85310632])) conspiracy\n",
      "(('__label__0',), array([0.86171168])) australia\n",
      "(('__label__0',), array([0.62897819])) casualiama\n",
      "(('__label__1',), array([0.97564864])) AutismGamingMemes\n",
      "(('__label__0',), array([0.67288291])) Ohio\n",
      "(('__label__1',), array([0.86228263])) AskNT\n",
      "(('__label__0',), array([0.6081121])) FCMemes\n",
      "(('__label__0',), array([0.63740134])) worldnew\n",
      "(('__label__0',), array([0.83142865])) actuallesbians\n",
      "(('__label__0',), array([0.94905686])) redscarepod\n",
      "(('__label__0',), array([0.88672936])) ADHD\n",
      "(('__label__0',), array([0.65753025])) AGPInAction\n",
      "(('__label__1',), array([0.72499925])) undiagnosedADHDandASD\n",
      "(('__label__0',), array([0.8521744])) TrueOffMyChest\n",
      "(('__label__1',), array([0.76066244])) downsyndrome\n",
      "(('__label__0',), array([0.9133606])) disability\n",
      "(('__label__1',), array([0.54774797])) Hypophosphatasia\n",
      "(('__label__1',), array([0.94886816])) AutismSwears\n",
      "(('__label__0',), array([0.73641092])) AspieGaming\n",
      "(('__label__0',), array([0.79274011])) aspergers_Beta\n",
      "(('__label__1',), array([0.9980157])) Autism_Advice\n",
      "(('__label__0',), array([0.74289447])) CMH\n",
      "(('__label__1',), array([0.99815261])) AutismSpeaksSucks\n",
      "(('__label__1',), array([0.54406208])) strandedonsol3\n",
      "(('__label__0',), array([0.72871417])) relationship_advice\n",
      "(('__label__0',), array([0.79373276])) Atypical\n",
      "(('__label__0',), array([0.89450169])) Advice\n",
      "(('__label__0',), array([0.87442338])) asktransgender\n",
      "(('__label__1',), array([0.87980944])) AmIAutistic\n",
      "(('__label__0',), array([0.83152956])) gaming\n",
      "(('__label__1',), array([0.96607333])) ndwitches\n",
      "(('__label__1',), array([0.55828702])) AutismCertified\n",
      "(('__label__0',), array([0.74562323])) OSDD\n",
      "(('__label__0',), array([0.5218693])) NAMItalk\n",
      "(('__label__0',), array([0.60468763])) insanepeoplefacebook\n",
      "(('__label__0',), array([0.54881328])) voidpunk\n",
      "(('__label__0',), array([0.75809026])) Hamilton\n",
      "(('__label__1',), array([0.90197307])) AutisticQueers\n",
      "(('__label__0',), array([0.65346771])) mentalillness\n",
      "(('__label__1',), array([0.91050661])) AuTeens\n",
      "(('__label__0',), array([0.6789071])) aromantic\n",
      "(('__label__0',), array([0.82351679])) DrugNerds\n",
      "(('__label__1',), array([0.98194379])) Aspergians\n",
      "(('__label__1',), array([0.94735479])) Aspergers_Elders\n",
      "(('__label__0',), array([0.66162956])) toastme\n",
      "(('__label__1',), array([0.99766356])) MomsWithAutism\n",
      "(('__label__1',), array([0.72032875])) LoveOnTheSpectrumShow\n",
      "(('__label__1',), array([0.79021889])) AspieMoms\n",
      "(('__label__0',), array([0.7369647])) Parenting\n",
      "(('__label__1',), array([0.93104082])) Cookingforautistics\n",
      "(('__label__1',), array([0.5679059])) GirlGuidetoAutism\n",
      "(('__label__0',), array([0.79831803])) skeptic\n",
      "(('__label__0',), array([0.98850399])) mentalhealth\n",
      "(('__label__0',), array([0.86939609])) aweism\n",
      "(('__label__0',), array([0.54447573])) fakedisordercringe\n",
      "(('__label__0',), array([0.92354554])) worldnews\n",
      "(('__label__1',), array([0.99555093])) AutisticParents\n",
      "(('__label__1',), array([0.74636424])) ASDcareers\n",
      "(('__label__1',), array([0.50649339])) YoungEntrepreneurs\n",
      "(('__label__0',), array([0.83822531])) PostHardcore\n",
      "(('__label__0',), array([0.89944649])) circlejerk\n",
      "(('__label__0',), array([0.93560839])) BabyBumps\n",
      "(('__label__1',), array([0.9355042])) The_Autism_Dad_Pod\n",
      "(('__label__0',), array([0.90423858])) slp\n",
      "(('__label__0',), array([0.76596332])) Microbiome\n",
      "(('__label__0',), array([0.59665138])) Aspergers_Dating_\n",
      "(('__label__0',), array([0.99408084])) ketoscience\n",
      "(('__label__1',), array([0.65338045])) aspergersnetwork\n",
      "(('__label__1',), array([0.99302024])) AutisticBirders\n",
      "(('__label__0',), array([0.6787951])) Teachers\n",
      "(('__label__1',), array([0.98332578])) AutisticPerspective\n",
      "(('__label__1',), array([0.6709826])) ASD_Advice\n",
      "(('__label__1',), array([0.99999905])) AutismWithADHD\n",
      "(('__label__1',), array([0.99999356])) ASDr4r\n",
      "(('__label__0',), array([0.71782386])) 196\n",
      "(('__label__1',), array([0.66661322])) aplusguide\n",
      "(('__label__1',), array([0.94069624])) lowfunctioning\n",
      "(('__label__1',), array([0.85468096])) AutismWithinWomen\n",
      "(('__label__1',), array([0.99941945])) ProAutism\n",
      "(('__label__0',), array([0.97659886])) Schizoid\n",
      "(('__label__0',), array([0.83872968])) Epilepsy\n",
      "(('__label__0',), array([0.74545383])) peercoin\n",
      "(('__label__1',), array([1.00000942])) Autism_india\n",
      "(('__label__1',), array([0.99832761])) AutisticTherapist\n",
      "(('__label__0',), array([0.5314793])) EGBOKfreeform\n",
      "(('__label__1',), array([0.96739674])) InsaneVegans\n",
      "(('__label__0',), array([0.96545929])) psychology\n",
      "(('__label__1',), array([1.00000989])) weaponize_autism\n",
      "(('__label__0',), array([0.83586067])) NDIS\n",
      "(('__label__0',), array([0.79244775])) Mycobiome\n",
      "(('__label__0',), array([0.70413876])) CompulsiveSkinPicking\n",
      "(('__label__1',), array([0.9848485])) Autistic_Gamers\n",
      "(('__label__0',), array([0.65809512])) psychobiotics\n",
      "(('__label__1',), array([0.56789088])) NeurotypicalStudies\n",
      "(('__label__0',), array([0.65636206])) autismUK\n",
      "(('__label__1',), array([0.74976933])) raisedbyautistics\n",
      "(('__label__0',), array([0.58775556])) toddlers\n",
      "(('__label__0',), array([0.80230641])) Savant\n",
      "(('__label__0',), array([0.69256282])) askgaybros\n",
      "(('__label__0',), array([0.79507405])) polls\n",
      "(('__label__1',), array([0.99921745])) AutisticMemes\n",
      "(('__label__1',), array([0.99794686])) AutisticAces\n"
     ]
    }
   ],
   "source": [
    "# classify all subreddits\n",
    "for index, row in asd_subs_df.iterrows():\n",
    "    print(ft_model.predict(row.text), row.display_name)\n",
    "    \n",
    "    prediction = ft_model.predict(row.text)\n",
    "    # get the predicted label\n",
    "    pred_label = prediction[0][0] if prediction[0][0].startswith(\"__label__\") else prediction[0][0]\n",
    "    # insert the predicted label into the df\n",
    "    asd_subs_df.at[index, 'predicted_label'] = pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 526 entries, 0 to 533\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               526 non-null    object\n",
      " 1   display_name     526 non-null    object\n",
      " 2   text             526 non-null    object\n",
      " 3   lang             526 non-null    object\n",
      " 4   predicted_label  526 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 40.8+ KB\n"
     ]
    }
   ],
   "source": [
    "asd_subs_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Representation Autism Awareness A place...</td>\n",
       "      <td>en</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>Aspie Positive Aspie Positive by aspies for as...</td>\n",
       "      <td>en</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism Trump Autism Trump Autism Trump A...</td>\n",
       "      <td>lv</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ltpt</td>\n",
       "      <td>nevergrewup</td>\n",
       "      <td>nevergrewup When the body got older but the mi...</td>\n",
       "      <td>en</td>\n",
       "      <td>__label__0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2s6ou</td>\n",
       "      <td>asperger</td>\n",
       "      <td>asperger Asperger Subreddit Unlike other autis...</td>\n",
       "      <td>en</td>\n",
       "      <td>__label__1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "3   3ltpt           nevergrewup   \n",
       "4   2s6ou              asperger   \n",
       "\n",
       "                                                text lang predicted_label  \n",
       "0  Autism Representation Autism Awareness A place...   en      __label__1  \n",
       "1  Aspie Positive Aspie Positive by aspies for as...   en      __label__1  \n",
       "2  Trump Autism Trump Autism Trump Autism Trump A...   lv      __label__1  \n",
       "3  nevergrewup When the body got older but the mi...   en      __label__0  \n",
       "4  asperger Asperger Subreddit Unlike other autis...   en      __label__1  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predicted_label\n",
       "ASD      311\n",
       "Other    215\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change the label to the category name\n",
    "label_map = {'__label__1': 'ASD', '__label__0': 'Other'}\n",
    "asd_subs_df['predicted_label'] = asd_subs_df['predicted_label'].map(label_map)\n",
    "label_counts = asd_subs_df['predicted_label'].value_counts()\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>display_name</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28lhh2</td>\n",
       "      <td>AutismRepresentation</td>\n",
       "      <td>Autism Representation Autism Awareness A place...</td>\n",
       "      <td>en</td>\n",
       "      <td>ASD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3b4rh</td>\n",
       "      <td>AspiePositive</td>\n",
       "      <td>Aspie Positive Aspie Positive by aspies for as...</td>\n",
       "      <td>en</td>\n",
       "      <td>ASD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2nwjjb</td>\n",
       "      <td>TrumpAutism</td>\n",
       "      <td>Trump Autism Trump Autism Trump Autism Trump A...</td>\n",
       "      <td>lv</td>\n",
       "      <td>ASD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3ltpt</td>\n",
       "      <td>nevergrewup</td>\n",
       "      <td>nevergrewup When the body got older but the mi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2s6ou</td>\n",
       "      <td>asperger</td>\n",
       "      <td>asperger Asperger Subreddit Unlike other autis...</td>\n",
       "      <td>en</td>\n",
       "      <td>ASD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id          display_name  \\\n",
       "0  28lhh2  AutismRepresentation   \n",
       "1   3b4rh         AspiePositive   \n",
       "2  2nwjjb           TrumpAutism   \n",
       "3   3ltpt           nevergrewup   \n",
       "4   2s6ou              asperger   \n",
       "\n",
       "                                                text lang predicted_label  \n",
       "0  Autism Representation Autism Awareness A place...   en             ASD  \n",
       "1  Aspie Positive Aspie Positive by aspies for as...   en             ASD  \n",
       "2  Trump Autism Trump Autism Trump Autism Trump A...   lv             ASD  \n",
       "3  nevergrewup When the body got older but the mi...   en           Other  \n",
       "4  asperger Asperger Subreddit Unlike other autis...   en             ASD  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd_subs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot \n",
    "plt.bar(label_counts.index, label_counts.values)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Predicted Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 311 entries, 0 to 533\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               311 non-null    object\n",
      " 1   display_name     311 non-null    object\n",
      " 2   text             311 non-null    object\n",
      " 3   lang             311 non-null    object\n",
      " 4   predicted_label  311 non-null    object\n",
      "dtypes: object(5)\n",
      "memory usage: 14.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# the model predictied 311 subreddit as being in the class ASD\n",
    "# dorp the irelevant subreddits ( in the Other category )\n",
    "asd_subs_df_final = asd_subs_df[asd_subs_df['predicted_label'] != 'Other']\n",
    "asd_subs_df_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the final dataset\n",
    "file_path = 'data/final_asd_subreddits_list.csv'\n",
    "asd_subs_df_final.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
